{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "202006-4-wipに向けて.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPczBQ+fRBkZqu1NpNoUZSH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bekku/deeplearning/blob/master/202006_4_wip%E3%81%AB%E5%90%91%E3%81%91%E3%81%A6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xfH4pVxedm_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d60006f5-7690-4a7f-ee3e-82bc304442c7"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as f\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "from google.colab import drive #インポート\n",
        "drive.mount('/content/gdrive') #GoogleDriveのマウント\n",
        "\n",
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3c_Pe5GEEMt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_cifar10(batch=128):\n",
        "    train_loader = DataLoader(\n",
        "        datasets.CIFAR10('./data',\n",
        "                         train=True,\n",
        "                         download=True,\n",
        "                         transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize(\n",
        "                                [0.5, 0.5, 0.5],  # RGB 平均\n",
        "                                [0.5, 0.5, 0.5]   # RGB 標準偏差\n",
        "                                )\n",
        "                         ])),\n",
        "        batch_size=batch,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        datasets.CIFAR10('./data',\n",
        "                         train=False,\n",
        "                         download=True,\n",
        "                         transform=transforms.Compose([\n",
        "                             transforms.ToTensor(),\n",
        "                             transforms.Normalize(\n",
        "                                 [0.5, 0.5, 0.5],  # RGB 平均\n",
        "                                 [0.5, 0.5, 0.5]  # RGB 標準偏差\n",
        "                             )\n",
        "                         ])),\n",
        "        batch_size=batch,\n",
        "    )\n",
        "\n",
        "    return {'train': train_loader, 'test': test_loader}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSuojR8LefpG",
        "colab_type": "text"
      },
      "source": [
        "# **与えられたリストからNNモデルの生成関数**\n",
        "\n",
        "### 問題\n",
        "\n",
        "あるN + 1 個の要素を持つ数列 A が与えられる。高橋くんは、与えられた数列の情報から隠れ層が N 個となるNNモデルを作成したい。数列のA1は、N である。\n",
        "また、Aiは各NNモデルの隠れ層iのノード数となる。\n",
        "それを満たすNNモデルを生成せよ。\n",
        "NNの入力層のノード数は3×32×32であり、出力層のノード数は10である。\n",
        "\n",
        "### 制約\n",
        "\n",
        "1 <= N <= INF\n",
        "\n",
        "### 入力\n",
        "\n",
        "[ N , A1 , A2 , , , , , AN ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNd48_RBeq1j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CreateNet1(torch.nn.Module):\n",
        "    def __init__(self, NN_model_list):\n",
        "        super(CreateNet, self).__init__()\n",
        "\n",
        "        for i in range(NN_model_list[0]):\n",
        "            if i==0:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(3 * 32 * 32, NN_model_list[1])\n",
        "                #入力層\n",
        "            else:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(NN_model_list[i], NN_model_list[i+1])\n",
        "                #出力層 fcリストの個数は、常にNN_model_list[0]+1である。\n",
        "        self._modules[\"fc\"+str(i+2)]= torch.nn.Linear(NN_model_list[-1], 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self._modules)-1):\n",
        "            x = f.relu(self._modules[\"fc\"+str(i+1)](x))\n",
        "        x = self._modules[\"fc\"+str(len(self._modules))](x)\n",
        "        return f.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nt0u4rkTHLUe",
        "colab_type": "text"
      },
      "source": [
        "# **与えられたリストからNNモデルの生成関数2**\n",
        "\n",
        "### 問題\n",
        "\n",
        "ある N 個の要素を持つ数列 A が与えられる。高橋くんは、与えられた数列の情報から、NNモデルを作成したい。\n",
        "Aiは各NNモデルの隠れ層 i のノード数となる。隠れ層 i のノード数が0となる場合、その層 i は層とみなさないものとして生成する。\n",
        "上記を満たす、NNモデルを生成せよ。\n",
        "NNの入力層のノード数は3×32×32であり、出力層のノード数は10である。\n",
        "\n",
        "### 制約\n",
        "\n",
        "1 <= N <= INF\n",
        "\n",
        "### 入力\n",
        "\n",
        "[ A1 , A2 , , , , , AN ]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mH9coJsmI3ei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CreateNet(torch.nn.Module):\n",
        "    def __init__(self, NN_model_list):\n",
        "        super(CreateNet, self).__init__()\n",
        "\n",
        "        self.NN_model_list_2=[]\n",
        "        for i in range(len(NN_model_list)):\n",
        "            if NN_model_list[i]!=0:\n",
        "                self.NN_model_list_2.append(NN_model_list[i])\n",
        "\n",
        "        NN_model_list=[len(self.NN_model_list_2)]+self.NN_model_list_2\n",
        "\n",
        "        for i in range(len(NN_model_list)-1):\n",
        "            if i==0:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(3 * 32 * 32, NN_model_list[1])\n",
        "                #入力層\n",
        "            else:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(NN_model_list[i], NN_model_list[i+1])\n",
        "                #出力層 fcリストの個数は、常にNN_model_list[0]+1である。\n",
        "        self._modules[\"fc\"+str(i+2)]= torch.nn.Linear(NN_model_list[-1], 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for i in range(len(self._modules)-1):\n",
        "            x = f.relu(self._modules[\"fc\"+str(i+1)](x))\n",
        "        x = self._modules[\"fc\"+str(len(self._modules))](x)\n",
        "        return f.log_softmax(x, dim=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WH4cDtXSs8r2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "50fac5ab-b30a-46ad-c9c8-28b3c4872ea3"
      },
      "source": [
        "#実際に作られました\n",
        "\n",
        "NN_model_list=[3,1024,1024,256]\n",
        "# print(*CreateNet(NN_model_list).parameters())\n",
        "# print(vars(CreateNet(NN_model_list))\n",
        "# ValueError: optimizer got an empty parameter list!の対策として、paraの確認。\n",
        "# 下のvarsによって、クラスの変数を列挙して、モデルのパラに重要なorderdictが_modulesに入ることになる。\n",
        "\n",
        "NN_model_list=[1024,256]\n",
        "print(CreateNet(NN_model_list))\n",
        "# NN_model_list=[1,1024]\n",
        "# print(CreateNet(NN_model_list))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CreateNet(\n",
            "  (fc1): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
            "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JHujYwqXcWY",
        "colab_type": "text"
      },
      "source": [
        "生成モデルを動かしてみた。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC-iqeQ89dD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "055d74a3-a5eb-44cf-e5da-0062e4d08dfc"
      },
      "source": [
        "from torchsummary import summary\n",
        "NN_model_list=[1024,1024,256]\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    epoch = 50\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'test_loss': [],\n",
        "        'test_acc': [],\n",
        "    }\n",
        "\n",
        "    net: torch.nn.Module = CreateNet(NN_model_list)\n",
        "    loaders = load_cifar10()\n",
        "\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "\n",
        "    for e in range(epoch):\n",
        "\n",
        "        \"\"\" Training Part\"\"\"\n",
        "        loss = None\n",
        "        # 学習開始 (再開)\n",
        "        net.train(True)  \n",
        "        for i, (data, target) in enumerate(loaders['train']):\n",
        "            data = data.to(device)  # to GPU?\n",
        "            target = target.to(device)\n",
        "            data = data.view(-1, 32 * 32 * 3)\n",
        "            optimizer.zero_grad()\n",
        "            output = net(data)\n",
        "            loss = f.nll_loss(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if i % 90 == 0:\n",
        "                print('Training log: {} epoch ({} / 50000 train. data). Loss: {}'.format(e+1,\n",
        "                                                                                         (i+1)*128,\n",
        "                                                                                         loss.item())\n",
        "                      )\n",
        "        history['train_loss'].append(loss)\n",
        "        \n",
        "        \"\"\" Test Part \"\"\"\n",
        "        # 学習のストップ\n",
        "        net.eval()\n",
        "        test_loss = 0\n",
        "        correct = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for data, target in loaders['test']:\n",
        "                data = data.to(device)  # to GPU?\n",
        "                target = target.to(device)\n",
        "                data = data.view(-1, 32 * 32 * 3)\n",
        "                output = net(data)\n",
        "                test_loss += f.nll_loss(output, target, reduction='sum').item()                \n",
        "                pred = output.argmax(dim=1, keepdim=True)\n",
        "                \n",
        "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "                #eqっていうのは、同じ値だとTrueをとる。\n",
        "\n",
        "        test_loss /= 10000\n",
        "\n",
        "        print('Test loss (avg): {}, Accuracy: {}'.format(test_loss,\n",
        "                                                         correct / 10000))\n",
        "\n",
        "        history['test_loss'].append(test_loss)\n",
        "        history['test_acc'].append(correct / 10000)\n",
        "        \n",
        "    #====== 保存 =======\n",
        "    path = \"/content/gdrive/My Drive/コード/202006-4周目/rundom_model_weight.pth\" # 保存先pathの設定\n",
        "    torch.save(net.state_dict(), path) # 保存\n",
        "\n",
        "    # 結果の出力と描画\n",
        "    print(history)\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epoch+1), history['train_loss'], label='train_loss')\n",
        "    plt.plot(range(1, epoch+1), history['test_loss'], label='test_loss')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.legend()\n",
        "    plt.savefig('loss.png')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epoch+1), history['test_acc'])\n",
        "    plt.title('test accuracy')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.savefig('test_acc.png')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss (avg): 2.092606781387329, Accuracy: 0.5381\n",
            "Training log: 14 epoch (128 / 50000 train. data). Loss: 0.3713621199131012\n",
            "Training log: 14 epoch (11648 / 50000 train. data). Loss: 0.4288032650947571\n",
            "Training log: 14 epoch (23168 / 50000 train. data). Loss: 0.3333142101764679\n",
            "Training log: 14 epoch (34688 / 50000 train. data). Loss: 0.4205192029476166\n",
            "Training log: 14 epoch (46208 / 50000 train. data). Loss: 0.44838231801986694\n",
            "Test loss (avg): 2.286080798912048, Accuracy: 0.5422\n",
            "Training log: 15 epoch (128 / 50000 train. data). Loss: 0.2954057455062866\n",
            "Training log: 15 epoch (11648 / 50000 train. data). Loss: 0.44230884313583374\n",
            "Training log: 15 epoch (23168 / 50000 train. data). Loss: 0.3482576608657837\n",
            "Training log: 15 epoch (34688 / 50000 train. data). Loss: 0.28801873326301575\n",
            "Training log: 15 epoch (46208 / 50000 train. data). Loss: 0.35611245036125183\n",
            "Test loss (avg): 2.4215006090164186, Accuracy: 0.5476\n",
            "Training log: 16 epoch (128 / 50000 train. data). Loss: 0.37874576449394226\n",
            "Training log: 16 epoch (11648 / 50000 train. data). Loss: 0.2446729987859726\n",
            "Training log: 16 epoch (23168 / 50000 train. data). Loss: 0.23813970386981964\n",
            "Training log: 16 epoch (34688 / 50000 train. data). Loss: 0.50467848777771\n",
            "Training log: 16 epoch (46208 / 50000 train. data). Loss: 0.3265891671180725\n",
            "Test loss (avg): 2.429877875328064, Accuracy: 0.5459\n",
            "Training log: 17 epoch (128 / 50000 train. data). Loss: 0.20982259511947632\n",
            "Training log: 17 epoch (11648 / 50000 train. data). Loss: 0.27234312891960144\n",
            "Training log: 17 epoch (23168 / 50000 train. data). Loss: 0.21536663174629211\n",
            "Training log: 17 epoch (34688 / 50000 train. data). Loss: 0.23268760740756989\n",
            "Training log: 17 epoch (46208 / 50000 train. data). Loss: 0.28155168890953064\n",
            "Test loss (avg): 2.6122455781936647, Accuracy: 0.5361\n",
            "Training log: 18 epoch (128 / 50000 train. data). Loss: 0.24239899218082428\n",
            "Training log: 18 epoch (11648 / 50000 train. data). Loss: 0.23177288472652435\n",
            "Training log: 18 epoch (23168 / 50000 train. data). Loss: 0.21645277738571167\n",
            "Training log: 18 epoch (34688 / 50000 train. data). Loss: 0.26920318603515625\n",
            "Training log: 18 epoch (46208 / 50000 train. data). Loss: 0.20576895773410797\n",
            "Test loss (avg): 2.7359454410552977, Accuracy: 0.5436\n",
            "Training log: 19 epoch (128 / 50000 train. data). Loss: 0.15785109996795654\n",
            "Training log: 19 epoch (11648 / 50000 train. data). Loss: 0.2291010022163391\n",
            "Training log: 19 epoch (23168 / 50000 train. data). Loss: 0.4238847494125366\n",
            "Training log: 19 epoch (34688 / 50000 train. data). Loss: 0.34553611278533936\n",
            "Training log: 19 epoch (46208 / 50000 train. data). Loss: 0.25444498658180237\n",
            "Test loss (avg): 2.7595738267898557, Accuracy: 0.5349\n",
            "Training log: 20 epoch (128 / 50000 train. data). Loss: 0.20793339610099792\n",
            "Training log: 20 epoch (11648 / 50000 train. data). Loss: 0.22336483001708984\n",
            "Training log: 20 epoch (23168 / 50000 train. data). Loss: 0.24309618771076202\n",
            "Training log: 20 epoch (34688 / 50000 train. data). Loss: 0.23339375853538513\n",
            "Training log: 20 epoch (46208 / 50000 train. data). Loss: 0.2936706840991974\n",
            "Test loss (avg): 2.9823394927978515, Accuracy: 0.536\n",
            "Training log: 21 epoch (128 / 50000 train. data). Loss: 0.11278045922517776\n",
            "Training log: 21 epoch (11648 / 50000 train. data). Loss: 0.24571052193641663\n",
            "Training log: 21 epoch (23168 / 50000 train. data). Loss: 0.3760930001735687\n",
            "Training log: 21 epoch (34688 / 50000 train. data). Loss: 0.19898879528045654\n",
            "Training log: 21 epoch (46208 / 50000 train. data). Loss: 0.3232291042804718\n",
            "Test loss (avg): 3.074152738189697, Accuracy: 0.5456\n",
            "Training log: 22 epoch (128 / 50000 train. data). Loss: 0.12960048019886017\n",
            "Training log: 22 epoch (11648 / 50000 train. data). Loss: 0.15636327862739563\n",
            "Training log: 22 epoch (23168 / 50000 train. data). Loss: 0.14188627898693085\n",
            "Training log: 22 epoch (34688 / 50000 train. data). Loss: 0.15933340787887573\n",
            "Training log: 22 epoch (46208 / 50000 train. data). Loss: 0.28088241815567017\n",
            "Test loss (avg): 3.139233204650879, Accuracy: 0.5302\n",
            "Training log: 23 epoch (128 / 50000 train. data). Loss: 0.16907532513141632\n",
            "Training log: 23 epoch (11648 / 50000 train. data). Loss: 0.21985888481140137\n",
            "Training log: 23 epoch (23168 / 50000 train. data). Loss: 0.2137550711631775\n",
            "Training log: 23 epoch (34688 / 50000 train. data). Loss: 0.31722524762153625\n",
            "Training log: 23 epoch (46208 / 50000 train. data). Loss: 0.19761177897453308\n",
            "Test loss (avg): 3.13887873916626, Accuracy: 0.5325\n",
            "Training log: 24 epoch (128 / 50000 train. data). Loss: 0.1666574478149414\n",
            "Training log: 24 epoch (11648 / 50000 train. data). Loss: 0.08560725301504135\n",
            "Training log: 24 epoch (23168 / 50000 train. data). Loss: 0.16570153832435608\n",
            "Training log: 24 epoch (34688 / 50000 train. data). Loss: 0.2775697410106659\n",
            "Training log: 24 epoch (46208 / 50000 train. data). Loss: 0.23402464389801025\n",
            "Test loss (avg): 3.3322239723205564, Accuracy: 0.5341\n",
            "Training log: 25 epoch (128 / 50000 train. data). Loss: 0.15168996155261993\n",
            "Training log: 25 epoch (11648 / 50000 train. data). Loss: 0.09004546701908112\n",
            "Training log: 25 epoch (23168 / 50000 train. data). Loss: 0.0876753181219101\n",
            "Training log: 25 epoch (34688 / 50000 train. data). Loss: 0.14007163047790527\n",
            "Training log: 25 epoch (46208 / 50000 train. data). Loss: 0.22637756168842316\n",
            "Test loss (avg): 3.3534742946624756, Accuracy: 0.5429\n",
            "Training log: 26 epoch (128 / 50000 train. data). Loss: 0.18794061243534088\n",
            "Training log: 26 epoch (11648 / 50000 train. data). Loss: 0.17627577483654022\n",
            "Training log: 26 epoch (23168 / 50000 train. data). Loss: 0.2683817148208618\n",
            "Training log: 26 epoch (34688 / 50000 train. data). Loss: 0.10476768761873245\n",
            "Training log: 26 epoch (46208 / 50000 train. data). Loss: 0.18175403773784637\n",
            "Test loss (avg): 3.5655616119384765, Accuracy: 0.5265\n",
            "Training log: 27 epoch (128 / 50000 train. data). Loss: 0.2134193778038025\n",
            "Training log: 27 epoch (11648 / 50000 train. data). Loss: 0.20006784796714783\n",
            "Training log: 27 epoch (23168 / 50000 train. data). Loss: 0.2253299355506897\n",
            "Training log: 27 epoch (34688 / 50000 train. data). Loss: 0.12742175161838531\n",
            "Training log: 27 epoch (46208 / 50000 train. data). Loss: 0.2653504014015198\n",
            "Test loss (avg): 3.351565789794922, Accuracy: 0.5406\n",
            "Training log: 28 epoch (128 / 50000 train. data). Loss: 0.14137211441993713\n",
            "Training log: 28 epoch (11648 / 50000 train. data). Loss: 0.2288152575492859\n",
            "Training log: 28 epoch (23168 / 50000 train. data). Loss: 0.17979469895362854\n",
            "Training log: 28 epoch (34688 / 50000 train. data). Loss: 0.20217524468898773\n",
            "Training log: 28 epoch (46208 / 50000 train. data). Loss: 0.10110848397016525\n",
            "Test loss (avg): 3.580752637672424, Accuracy: 0.5394\n",
            "Training log: 29 epoch (128 / 50000 train. data). Loss: 0.0560401976108551\n",
            "Training log: 29 epoch (11648 / 50000 train. data). Loss: 0.1100146621465683\n",
            "Training log: 29 epoch (23168 / 50000 train. data). Loss: 0.2932690680027008\n",
            "Training log: 29 epoch (34688 / 50000 train. data). Loss: 0.22252190113067627\n",
            "Training log: 29 epoch (46208 / 50000 train. data). Loss: 0.1813448965549469\n",
            "Test loss (avg): 3.628742039871216, Accuracy: 0.5317\n",
            "Training log: 30 epoch (128 / 50000 train. data). Loss: 0.16719669103622437\n",
            "Training log: 30 epoch (11648 / 50000 train. data). Loss: 0.1587844341993332\n",
            "Training log: 30 epoch (23168 / 50000 train. data). Loss: 0.10937515646219254\n",
            "Training log: 30 epoch (34688 / 50000 train. data). Loss: 0.11518913507461548\n",
            "Training log: 30 epoch (46208 / 50000 train. data). Loss: 0.2207283079624176\n",
            "Test loss (avg): 3.4704596687316895, Accuracy: 0.5406\n",
            "Training log: 31 epoch (128 / 50000 train. data). Loss: 0.06506650894880295\n",
            "Training log: 31 epoch (11648 / 50000 train. data). Loss: 0.09818917512893677\n",
            "Training log: 31 epoch (23168 / 50000 train. data). Loss: 0.1950744092464447\n",
            "Training log: 31 epoch (34688 / 50000 train. data). Loss: 0.11297859251499176\n",
            "Training log: 31 epoch (46208 / 50000 train. data). Loss: 0.32484450936317444\n",
            "Test loss (avg): 3.7318563465118406, Accuracy: 0.5368\n",
            "Training log: 32 epoch (128 / 50000 train. data). Loss: 0.11077277362346649\n",
            "Training log: 32 epoch (11648 / 50000 train. data). Loss: 0.056149132549762726\n",
            "Training log: 32 epoch (23168 / 50000 train. data). Loss: 0.07056938111782074\n",
            "Training log: 32 epoch (34688 / 50000 train. data). Loss: 0.18578849732875824\n",
            "Training log: 32 epoch (46208 / 50000 train. data). Loss: 0.16256210207939148\n",
            "Test loss (avg): 3.910696332168579, Accuracy: 0.5334\n",
            "Training log: 33 epoch (128 / 50000 train. data). Loss: 0.055410727858543396\n",
            "Training log: 33 epoch (11648 / 50000 train. data). Loss: 0.17327745258808136\n",
            "Training log: 33 epoch (23168 / 50000 train. data). Loss: 0.2726259231567383\n",
            "Training log: 33 epoch (34688 / 50000 train. data). Loss: 0.15399852395057678\n",
            "Training log: 33 epoch (46208 / 50000 train. data). Loss: 0.13254062831401825\n",
            "Test loss (avg): 3.8051370903015136, Accuracy: 0.5375\n",
            "Training log: 34 epoch (128 / 50000 train. data). Loss: 0.08551102876663208\n",
            "Training log: 34 epoch (11648 / 50000 train. data). Loss: 0.22141894698143005\n",
            "Training log: 34 epoch (23168 / 50000 train. data). Loss: 0.27689093351364136\n",
            "Training log: 34 epoch (34688 / 50000 train. data). Loss: 0.1907486617565155\n",
            "Training log: 34 epoch (46208 / 50000 train. data). Loss: 0.16855502128601074\n",
            "Test loss (avg): 3.8719496768951416, Accuracy: 0.5349\n",
            "Training log: 35 epoch (128 / 50000 train. data). Loss: 0.10721393674612045\n",
            "Training log: 35 epoch (11648 / 50000 train. data). Loss: 0.16087353229522705\n",
            "Training log: 35 epoch (23168 / 50000 train. data). Loss: 0.15952542424201965\n",
            "Training log: 35 epoch (34688 / 50000 train. data). Loss: 0.11008525639772415\n",
            "Training log: 35 epoch (46208 / 50000 train. data). Loss: 0.20084279775619507\n",
            "Test loss (avg): 3.9385450996398927, Accuracy: 0.537\n",
            "Training log: 36 epoch (128 / 50000 train. data). Loss: 0.12794412672519684\n",
            "Training log: 36 epoch (11648 / 50000 train. data). Loss: 0.09336347877979279\n",
            "Training log: 36 epoch (23168 / 50000 train. data). Loss: 0.1547902226448059\n",
            "Training log: 36 epoch (34688 / 50000 train. data). Loss: 0.1816284954547882\n",
            "Training log: 36 epoch (46208 / 50000 train. data). Loss: 0.21004030108451843\n",
            "Test loss (avg): 3.9562332550048827, Accuracy: 0.5344\n",
            "Training log: 37 epoch (128 / 50000 train. data). Loss: 0.1749165803194046\n",
            "Training log: 37 epoch (11648 / 50000 train. data). Loss: 0.3026908040046692\n",
            "Training log: 37 epoch (23168 / 50000 train. data). Loss: 0.11543536186218262\n",
            "Training log: 37 epoch (34688 / 50000 train. data). Loss: 0.05734436213970184\n",
            "Training log: 37 epoch (46208 / 50000 train. data). Loss: 0.13837511837482452\n",
            "Test loss (avg): 4.045353894042969, Accuracy: 0.5343\n",
            "Training log: 38 epoch (128 / 50000 train. data). Loss: 0.11699815839529037\n",
            "Training log: 38 epoch (11648 / 50000 train. data). Loss: 0.21789276599884033\n",
            "Training log: 38 epoch (23168 / 50000 train. data). Loss: 0.09581641852855682\n",
            "Training log: 38 epoch (34688 / 50000 train. data). Loss: 0.22413749992847443\n",
            "Training log: 38 epoch (46208 / 50000 train. data). Loss: 0.11392854154109955\n",
            "Test loss (avg): 4.013922204589844, Accuracy: 0.5345\n",
            "Training log: 39 epoch (128 / 50000 train. data). Loss: 0.09793100506067276\n",
            "Training log: 39 epoch (11648 / 50000 train. data). Loss: 0.15895184874534607\n",
            "Training log: 39 epoch (23168 / 50000 train. data). Loss: 0.07987462729215622\n",
            "Training log: 39 epoch (34688 / 50000 train. data). Loss: 0.13514631986618042\n",
            "Training log: 39 epoch (46208 / 50000 train. data). Loss: 0.29893794655799866\n",
            "Test loss (avg): 4.163716810226441, Accuracy: 0.5296\n",
            "Training log: 40 epoch (128 / 50000 train. data). Loss: 0.1018991470336914\n",
            "Training log: 40 epoch (11648 / 50000 train. data). Loss: 0.12590596079826355\n",
            "Training log: 40 epoch (23168 / 50000 train. data). Loss: 0.16609303653240204\n",
            "Training log: 40 epoch (34688 / 50000 train. data). Loss: 0.11738234758377075\n",
            "Training log: 40 epoch (46208 / 50000 train. data). Loss: 0.16110247373580933\n",
            "Test loss (avg): 4.354582915878296, Accuracy: 0.5307\n",
            "Training log: 41 epoch (128 / 50000 train. data). Loss: 0.14674066007137299\n",
            "Training log: 41 epoch (11648 / 50000 train. data). Loss: 0.07303935289382935\n",
            "Training log: 41 epoch (23168 / 50000 train. data). Loss: 0.19878661632537842\n",
            "Training log: 41 epoch (34688 / 50000 train. data). Loss: 0.14618873596191406\n",
            "Training log: 41 epoch (46208 / 50000 train. data). Loss: 0.028541959822177887\n",
            "Test loss (avg): 4.34970744934082, Accuracy: 0.5293\n",
            "Training log: 42 epoch (128 / 50000 train. data). Loss: 0.18659023940563202\n",
            "Training log: 42 epoch (11648 / 50000 train. data). Loss: 0.15861603617668152\n",
            "Training log: 42 epoch (23168 / 50000 train. data). Loss: 0.20221389830112457\n",
            "Training log: 42 epoch (34688 / 50000 train. data). Loss: 0.131775364279747\n",
            "Training log: 42 epoch (46208 / 50000 train. data). Loss: 0.13891121745109558\n",
            "Test loss (avg): 3.9683526180267332, Accuracy: 0.5394\n",
            "Training log: 43 epoch (128 / 50000 train. data). Loss: 0.10892926156520844\n",
            "Training log: 43 epoch (11648 / 50000 train. data). Loss: 0.029682183638215065\n",
            "Training log: 43 epoch (23168 / 50000 train. data). Loss: 0.20715448260307312\n",
            "Training log: 43 epoch (34688 / 50000 train. data). Loss: 0.0869603380560875\n",
            "Training log: 43 epoch (46208 / 50000 train. data). Loss: 0.053130391985177994\n",
            "Test loss (avg): 4.632858513641358, Accuracy: 0.5308\n",
            "Training log: 44 epoch (128 / 50000 train. data). Loss: 0.09762546420097351\n",
            "Training log: 44 epoch (11648 / 50000 train. data). Loss: 0.11487721651792526\n",
            "Training log: 44 epoch (23168 / 50000 train. data). Loss: 0.19080904126167297\n",
            "Training log: 44 epoch (34688 / 50000 train. data). Loss: 0.07996413111686707\n",
            "Training log: 44 epoch (46208 / 50000 train. data). Loss: 0.25347626209259033\n",
            "Test loss (avg): 4.272635402297974, Accuracy: 0.5417\n",
            "Training log: 45 epoch (128 / 50000 train. data). Loss: 0.048378389328718185\n",
            "Training log: 45 epoch (11648 / 50000 train. data). Loss: 0.15258324146270752\n",
            "Training log: 45 epoch (23168 / 50000 train. data). Loss: 0.09507113695144653\n",
            "Training log: 45 epoch (34688 / 50000 train. data). Loss: 0.14305554330348969\n",
            "Training log: 45 epoch (46208 / 50000 train. data). Loss: 0.25486767292022705\n",
            "Test loss (avg): 4.497604322052002, Accuracy: 0.5425\n",
            "Training log: 46 epoch (128 / 50000 train. data). Loss: 0.1332693099975586\n",
            "Training log: 46 epoch (11648 / 50000 train. data). Loss: 0.1689324676990509\n",
            "Training log: 46 epoch (23168 / 50000 train. data). Loss: 0.0768488198518753\n",
            "Training log: 46 epoch (34688 / 50000 train. data). Loss: 0.32290998101234436\n",
            "Training log: 46 epoch (46208 / 50000 train. data). Loss: 0.20173999667167664\n",
            "Test loss (avg): 4.321445668792725, Accuracy: 0.5359\n",
            "Training log: 47 epoch (128 / 50000 train. data). Loss: 0.12511949241161346\n",
            "Training log: 47 epoch (11648 / 50000 train. data). Loss: 0.15999053418636322\n",
            "Training log: 47 epoch (23168 / 50000 train. data). Loss: 0.040561825037002563\n",
            "Training log: 47 epoch (34688 / 50000 train. data). Loss: 0.12469419091939926\n",
            "Training log: 47 epoch (46208 / 50000 train. data). Loss: 0.0747607871890068\n",
            "Test loss (avg): 4.381919246673584, Accuracy: 0.5426\n",
            "Training log: 48 epoch (128 / 50000 train. data). Loss: 0.10362526774406433\n",
            "Training log: 48 epoch (11648 / 50000 train. data). Loss: 0.03293227776885033\n",
            "Training log: 48 epoch (23168 / 50000 train. data). Loss: 0.01383696123957634\n",
            "Training log: 48 epoch (34688 / 50000 train. data). Loss: 0.19130803644657135\n",
            "Training log: 48 epoch (46208 / 50000 train. data). Loss: 0.13219669461250305\n",
            "Test loss (avg): 4.640002810668945, Accuracy: 0.5377\n",
            "Training log: 49 epoch (128 / 50000 train. data). Loss: 0.15450210869312286\n",
            "Training log: 49 epoch (11648 / 50000 train. data). Loss: 0.1970493644475937\n",
            "Training log: 49 epoch (23168 / 50000 train. data). Loss: 0.13362663984298706\n",
            "Training log: 49 epoch (34688 / 50000 train. data). Loss: 0.1364935338497162\n",
            "Training log: 49 epoch (46208 / 50000 train. data). Loss: 0.11587278544902802\n",
            "Test loss (avg): 4.4261677734375, Accuracy: 0.5318\n",
            "Training log: 50 epoch (128 / 50000 train. data). Loss: 0.18560586869716644\n",
            "Training log: 50 epoch (11648 / 50000 train. data). Loss: 0.16609153151512146\n",
            "Training log: 50 epoch (23168 / 50000 train. data). Loss: 0.07471992820501328\n",
            "Training log: 50 epoch (34688 / 50000 train. data). Loss: 0.05980420112609863\n",
            "Training log: 50 epoch (46208 / 50000 train. data). Loss: 0.29974281787872314\n",
            "Test loss (avg): 4.39025238494873, Accuracy: 0.5346\n",
            "{'train_loss': [tensor(1.4197, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.4904, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.3990, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.3821, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.1677, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.0001, device='cuda:0', grad_fn=<NllLossBackward>), tensor(1.1049, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.8280, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.8576, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.8151, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.5850, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.7116, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4530, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3443, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.6228, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3425, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3994, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2965, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1708, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2055, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4049, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2089, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3088, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1704, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1288, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1274, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1936, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2081, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0911, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0406, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.4360, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3337, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1075, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1800, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2040, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2537, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1654, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1969, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1775, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1767, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1113, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1356, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.2505, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1149, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0934, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.3935, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.0634, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1130, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1445, device='cuda:0', grad_fn=<NllLossBackward>), tensor(0.1204, device='cuda:0', grad_fn=<NllLossBackward>)], 'test_loss': [1.4803662763595582, 1.4109193841934204, 1.3796393844604493, 1.3685577928543091, 1.3638931283950806, 1.3709284183502197, 1.4246143672943115, 1.4621472118377685, 1.5738771648406982, 1.6472884728431703, 1.8166895797729492, 1.931661344909668, 2.092606781387329, 2.286080798912048, 2.4215006090164186, 2.429877875328064, 2.6122455781936647, 2.7359454410552977, 2.7595738267898557, 2.9823394927978515, 3.074152738189697, 3.139233204650879, 3.13887873916626, 3.3322239723205564, 3.3534742946624756, 3.5655616119384765, 3.351565789794922, 3.580752637672424, 3.628742039871216, 3.4704596687316895, 3.7318563465118406, 3.910696332168579, 3.8051370903015136, 3.8719496768951416, 3.9385450996398927, 3.9562332550048827, 4.045353894042969, 4.013922204589844, 4.163716810226441, 4.354582915878296, 4.34970744934082, 3.9683526180267332, 4.632858513641358, 4.272635402297974, 4.497604322052002, 4.321445668792725, 4.381919246673584, 4.640002810668945, 4.4261677734375, 4.39025238494873], 'test_acc': [0.4797, 0.5076, 0.5169, 0.5267, 0.5369, 0.5486, 0.5436, 0.5487, 0.5436, 0.5407, 0.5441, 0.5455, 0.5381, 0.5422, 0.5476, 0.5459, 0.5361, 0.5436, 0.5349, 0.536, 0.5456, 0.5302, 0.5325, 0.5341, 0.5429, 0.5265, 0.5406, 0.5394, 0.5317, 0.5406, 0.5368, 0.5334, 0.5375, 0.5349, 0.537, 0.5344, 0.5343, 0.5345, 0.5296, 0.5307, 0.5293, 0.5394, 0.5308, 0.5417, 0.5425, 0.5359, 0.5426, 0.5377, 0.5318, 0.5346]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x7ZM9W-G-ISz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6b62ca05-0f47-4406-f436-80688a9df048"
      },
      "source": [
        "NNnet: CreateNet = CreateNet(NN_model_list)\n",
        "    \n",
        "# 保存したモデルのパラメータ\n",
        "param = torch.load('/content/gdrive/My Drive/コード/202006-4周目/rundom_model_weight.pth')\n",
        "\n",
        "# 保存したモデルにパラメータを当てはめる\n",
        "NNnet.load_state_dict(param)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkEI5sSaeftj",
        "colab_type": "text"
      },
      "source": [
        "# **正規化評価関数作成**\n",
        "\n",
        "引数を速度とaccuracyを取ります。\n",
        "\n",
        "速度の最小と最大も欲しい。(最小=0,最大=元モデル)\n",
        "\n",
        "精度の最小と最大も欲しい。(最小=0,最大=1 or 元モデル精度)\n",
        "\n",
        "**問題：精度0を許すと速度が振り切れるってのがあるから、改善の余地**\n",
        "\n",
        "速度は速いほど0.5に近づき、精度は高いほど0.5に近づく。\n",
        "\n",
        "合計で1.0が最高のパフォーマンス。ちなありえない。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ro7MWbvdlpZo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def speed_acc_normalization(spe_max,spe_min,spe,acc_max,acc_min,acc):\n",
        "    if spe_min<=spe<=spe_max:\n",
        "        speed_normalization = ((spe_max-spe+1) - spe_min) / ( (spe_max - spe_min) * 2 )\n",
        "    else:\n",
        "        speed_normalization=0\n",
        "    if acc_min<=acc<=acc_max:\n",
        "      accuracy_normalization = (acc - acc_min) / ( (acc_max - acc_min) * 2 )\n",
        "    else:\n",
        "        accuracy_normalization=0\n",
        "\n",
        "    print(speed_normalization)\n",
        "    print(accuracy_normalization)\n",
        "\n",
        "    return speed_normalization + accuracy_normalization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uugXvjjFm7FP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "loader = load_cifar10()\n",
        "t1 = time.time()\n",
        "\n",
        "\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in loader['test']:\n",
        "        data = images.view(-1, 32 * 32 * 3)\n",
        "        outputs = NNnet(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #torch.max(a, axis) (axis=0:col, axis=1:row)ごとに最大値を取ってくれている。\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "acc = float(correct / 10000)\n",
        "print(acc)\n",
        "\n",
        "\n",
        "t2 = time.time()\n",
        "elapsed_time = t2-t1\n",
        "print(f\"経過時間：{elapsed_time}\")\n",
        "\n",
        "print(speed_acc_normalization(5,0,elapsed_time,1,0,acc))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "27dyhdxJid8W",
        "colab_type": "text"
      },
      "source": [
        "## **教師モデルと温度付きソフトマックス**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POESyPKCW56q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "f8c3a954-26c3-462b-af9c-2f03063467dd"
      },
      "source": [
        "#教師モデル\n",
        "#####ドロップアウトを除いた推論用モデル#######\n",
        "#ここで温度を変更して、softターゲットとする。\n",
        "\n",
        "\n",
        "class T_CNN(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(T_CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = torch.nn.Conv2d(3, 32, 3, 1, 0)\n",
        "        self.conv2 = torch.nn.Conv2d(32, 32, 3)\n",
        "        self.conv3 = torch.nn.Conv2d(32, 64, 3)\n",
        "        self.conv4 = torch.nn.Conv2d(64, 64, 3)\n",
        "        \n",
        "        self.dropout1 = torch.nn.Dropout2d(p=0.25)\n",
        "        self.dropout2 = torch.nn.Dropout2d(p=0.5)\n",
        "\n",
        "        self.pool = torch.nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.fc1 = torch.nn.Linear(64 * 5 * 5, 512) \n",
        "        self.fc2 = torch.nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        x = f.relu(self.conv1(x))\n",
        "        x = f.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        \n",
        "        x = f.relu(self.conv3(x))\n",
        "        x = f.relu(self.conv4(x))\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 5 * 5)\n",
        "        x = f.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x/T\n",
        "\n",
        "        return f.softmax(x)\n",
        "\n",
        "class MyNet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(32 * 32 * 3, 1024)\n",
        "        self.fc2 = torch.nn.Linear(1024, 10)\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        x = f.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        x = x/T\n",
        "        return f.log_softmax(x)\n",
        "# 教師モデルと生徒モデルのcross_entropyの損失関数の定義\n",
        "class T_S_Closs_entropy(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(T_S_Closs_entropy, self).__init__()\n",
        "\n",
        "    def forward(self, p, q):\n",
        "        #pが教師モデル、qが生徒モデルの出力値(既に対数化してます)です。\n",
        "        # c = torch.max(q)\n",
        "        loss = p*q\n",
        "        loss = torch.sum(loss)\n",
        "        loss*=-1\n",
        "        return loss\n",
        "\n",
        "Tnet: T_CNN = T_CNN()\n",
        "param = torch.load('/content/gdrive/My Drive/コード/teacher_model_weight.pth')\n",
        "Tnet.load_state_dict(param)\n",
        "loader = load_cifar10()\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "Tnet.to(device)\n",
        "t1 = time.time()\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in loader['test']:\n",
        "        images = images.to(device) \n",
        "        labels = labels.to(device)\n",
        "        outputs = Tnet(images,1)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "a_max = float(correct / 10000)\n",
        "print(a_max)\n",
        "t2 = time.time()\n",
        "s_max = t2-t1\n",
        "print(f\"経過時間：{s_max}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.7364\n",
            "経過時間：2.9531991481781006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rc0MDh4LMGtX",
        "colab_type": "text"
      },
      "source": [
        "## **最終的に遺伝的探索を行うための評価関数**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hX2ZHcLfiBD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dd5a29c9-7cca-4f61-b1c2-cbb1e2bc04d5"
      },
      "source": [
        "# 温度付きモデル生成関数\n",
        "class CreateNet(torch.nn.Module):\n",
        "    def __init__(self, NN_model_list):\n",
        "        super(CreateNet, self).__init__()\n",
        "\n",
        "        self.NN_model_list_2=[]\n",
        "        for i in range(len(NN_model_list)):\n",
        "            if NN_model_list[i]!=0:\n",
        "                self.NN_model_list_2.append(NN_model_list[i])\n",
        "\n",
        "        NN_model_list=[len(self.NN_model_list_2)]+self.NN_model_list_2\n",
        "\n",
        "        for i in range(len(NN_model_list)-1):\n",
        "            if i==0:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(3 * 32 * 32, NN_model_list[1])\n",
        "                #入力層\n",
        "            else:\n",
        "                self._modules[\"fc\"+str(i+1)]= torch.nn.Linear(NN_model_list[i], NN_model_list[i+1])\n",
        "                #出力層 fcリストの個数は、常にNN_model_list[0]+1である。\n",
        "        self._modules[\"fc\"+str(i+2)]= torch.nn.Linear(NN_model_list[-1], 10)\n",
        "\n",
        "    def forward(self, x, T):\n",
        "        for i in range(len(self._modules)-1):\n",
        "            x = f.relu(self._modules[\"fc\"+str(i+1)](x))\n",
        "        x = self._modules[\"fc\"+str(len(self._modules))](x)\n",
        "        x = x/T\n",
        "        return f.log_softmax(x)\n",
        "\n",
        "# 速度と精度の評価関数これが最大化されたい。\n",
        "def speed_acc_normalization(spe_max,spe_min,spe,acc_max,acc_min,acc):\n",
        "    if spe_min<=spe<=spe_max:\n",
        "        speed_normalization = ((spe_max-spe+1) - spe_min) / ( (spe_max - spe_min) * 2 )\n",
        "    else:\n",
        "        speed_normalization=0\n",
        "    if acc_min<=acc<=acc_max:\n",
        "      accuracy_normalization = (acc - acc_min) / ( (acc_max - acc_min) * 2 )\n",
        "    else:\n",
        "        accuracy_normalization=0\n",
        "    print(\"----------------\")\n",
        "    print(\"速度:{} ,精度:{}\".format(spe,acc))\n",
        "    print(\"速度評価値:{} ,精度評価値:{}\".format(speed_normalization,accuracy_normalization))\n",
        "    print(\"評価指標:{}\".format(speed_normalization + accuracy_normalization))\n",
        "    print(\"----------------\")\n",
        "\n",
        "    return speed_normalization + accuracy_normalization\n",
        "\n",
        "\n",
        "\n",
        "#　遺伝的評価関数、引数 : gane=層ごとの引数-リスト、temp = 温度、スピード最大、精度最大。\n",
        "def evaluater(gane,temp,s_max,a_max=1.0):\n",
        "    Tnet: T_CNN = T_CNN()\n",
        "    # 保存したモデルのパラメータ\n",
        "    param = torch.load('/content/gdrive/My Drive/コード/teacher_model_weight.pth')\n",
        "    # 保存したモデルにパラメータを当てはめる\n",
        "    Tnet.load_state_dict(param)\n",
        "\n",
        "    temp=temp\n",
        "    # 学習回数\n",
        "    epoch = 50\n",
        "    net: torch.nn.Module = CreateNet(gane)\n",
        "    loaders = load_cifar10()\n",
        "\n",
        "    criterion = T_S_Closs_entropy()\n",
        "\n",
        "    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.001)\n",
        "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    net.to(device)\n",
        "    Tnet.to(device)\n",
        "\n",
        "    for e in range(1,epoch+1):\n",
        "        \"\"\"　学習　開始\"\"\"\n",
        "        loss = None\n",
        "        net.train()\n",
        "        for data, target in loaders['train']:\n",
        "                \n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            max_model_outputs = Tnet(data,temp)\n",
        "            data = data.view(-1, 32 * 32 * 3)\n",
        "            output = net(data,temp)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(max_model_outputs,output)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        if e%10==0:\n",
        "            print('temp: {} ,Training log: {} ,Loss: {}'.format(temp,e,loss.item()))\n",
        "\n",
        "    \"\"\"  学習したモデルの速度と精度の評価をする。(推論)\"\"\"\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    t1 = time.time()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loaders['test']:\n",
        "            images = images.to(device) \n",
        "            labels = labels.to(device)\n",
        "            data = images.view(-1, 32 * 32 * 3)\n",
        "            outputs = net(data,1)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            #torch.max(a, axis) (axis=0:col, axis=1:row)ごとに最大値を取ってくれている。\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    acc = float(correct / 10000)\n",
        "    t2 = time.time()\n",
        "    elapsed_time = t2-t1\n",
        "\n",
        "    return speed_acc_normalization(s_max,0,elapsed_time,a_max,0,acc)\n",
        "print(\"出力\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "出力\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6Y5BhFKMcsF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "43787190-2244-4203-c567-8e5db43bb579"
      },
      "source": [
        "eva_t = time.time()\n",
        "print(evaluater([1024,512] ,10 ,s_max ,a_max))\n",
        "eva_t2 = time.time()\n",
        "print(\"モデル1つ当たりの学習時間:{}\".format(eva_t2 -eva_t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temp: 10 ,Training log: 10 ,Loss: 178.7676544189453\n",
            "temp: 10 ,Training log: 20 ,Loss: 179.7474365234375\n",
            "temp: 10 ,Training log: 30 ,Loss: 179.8099365234375\n",
            "temp: 10 ,Training log: 40 ,Loss: 178.8997802734375\n",
            "temp: 10 ,Training log: 50 ,Loss: 178.4928741455078\n",
            "速度:2.6592020988464355 ,精度:0.5799\n",
            "速度評価値:0.2190839466633943 ,精度評価値:0.39373981531776203\n",
            "モデル1つ当たりの学習時間:744.362138748169\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lSLiPy1AoYRG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        },
        "outputId": "027c3eed-a75f-4f04-a1b6-a127bbaa5dac"
      },
      "source": [
        "eva_t = time.time()\n",
        "evaluater([1024] ,10 ,s_max ,a_max)\n",
        "eva_t2 = time.time()\n",
        "print(\"学習時間:{}\".format(eva_t2 -eva_t))\n",
        "\n",
        "eva_t = time.time()\n",
        "evaluater([1024,512] ,10 ,s_max ,a_max)\n",
        "eva_t2 = time.time()\n",
        "print(\"学習時間:{}\".format(eva_t2 -eva_t))\n",
        "\n",
        "eva_t = time.time()\n",
        "evaluater([2048,1024,512] ,10 ,s_max ,a_max)\n",
        "eva_t2 = time.time()\n",
        "print(\"学習時間:{}\".format(eva_t2 -eva_t))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:36: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "temp: 10 ,Training log: 10 ,Loss: 180.3373565673828\n",
            "temp: 10 ,Training log: 20 ,Loss: 179.5028839111328\n",
            "temp: 10 ,Training log: 30 ,Loss: 178.09298706054688\n",
            "temp: 10 ,Training log: 40 ,Loss: 178.81155395507812\n",
            "temp: 10 ,Training log: 50 ,Loss: 179.34249877929688\n",
            "----------------\n",
            "速度:2.6391704082489014 ,精度:0.5488\n",
            "速度評価値:0.22247547049779137 ,精度評価値:0.37262357414448666\n",
            "評価指標:0.595099044642278\n",
            "----------------\n",
            "0.595099044642278\n",
            "学習時間:730.113119840622\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "temp: 10 ,Training log: 10 ,Loss: 179.1807098388672\n",
            "temp: 10 ,Training log: 20 ,Loss: 178.33551025390625\n",
            "temp: 10 ,Training log: 30 ,Loss: 177.39285278320312\n",
            "temp: 10 ,Training log: 40 ,Loss: 179.2025146484375\n",
            "temp: 10 ,Training log: 50 ,Loss: 177.89041137695312\n",
            "----------------\n",
            "速度:2.5444624423980713 ,精度:0.5726\n",
            "速度評価値:0.23851027903910793 ,精度評価値:0.38878326996197715\n",
            "評価指標:0.6272935490010851\n",
            "----------------\n",
            "0.6272935490010851\n",
            "学習時間:737.5025703907013\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "temp: 10 ,Training log: 10 ,Loss: 178.07513427734375\n",
            "temp: 10 ,Training log: 20 ,Loss: 177.98178100585938\n",
            "temp: 10 ,Training log: 30 ,Loss: 178.70164489746094\n",
            "temp: 10 ,Training log: 40 ,Loss: 178.46636962890625\n",
            "temp: 10 ,Training log: 50 ,Loss: 178.3575897216797\n",
            "----------------\n",
            "速度:2.7454848289489746 ,精度:0.5874\n",
            "速度評価値:0.20447559724751274 ,精度評価値:0.39883215643671915\n",
            "評価指標:0.603307753684232\n",
            "----------------\n",
            "0.603307753684232\n",
            "学習時間:749.9553279876709\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4MTe7WIosCT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}