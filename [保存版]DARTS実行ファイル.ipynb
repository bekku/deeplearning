{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[保存版]DARTS実行ファイル.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMgYx97GkMkEXaBAIVpjevX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bekku/deeplearning/blob/master/%5B%E4%BF%9D%E5%AD%98%E7%89%88%5DDARTS%E5%AE%9F%E8%A1%8C%E3%83%95%E3%82%A1%E3%82%A4%E3%83%AB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YSoZMZ1xsYR",
        "outputId": "8bbefeb6-a273-4b32-bcd2-bb19b74424b8"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan  8 12:23:28 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.27.04    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    24W / 300W |      0MiB / 16130MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fszfBROTXbOr"
      },
      "source": [
        "from torchsummary import summary"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yJp2T_b2lRY_"
      },
      "source": [
        "# **utils.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiQPBEKQhajL"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import shutil\n",
        "import torchvision.transforms as transforms\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class AvgrageMeter(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    self.reset()\n",
        "\n",
        "  def reset(self):\n",
        "    self.avg = 0\n",
        "    self.sum = 0\n",
        "    self.cnt = 0\n",
        "\n",
        "  def update(self, val, n=1):\n",
        "    self.sum += val * n\n",
        "    self.cnt += n\n",
        "    self.avg = self.sum / self.cnt\n",
        "\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "  maxk = max(topk)\n",
        "  batch_size = target.size(0)\n",
        "\n",
        "  _, pred = output.topk(maxk, 1, True, True)\n",
        "  pred = pred.t()\n",
        "  correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "  res = []\n",
        "  for k in topk:\n",
        "    correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "    ########### view　→ reshape  ###########\n",
        "    res.append(correct_k.mul_(100.0/batch_size))\n",
        "  return res\n",
        "\n",
        "\n",
        "class Cutout(object):\n",
        "    def __init__(self, length):\n",
        "        self.length = length\n",
        "\n",
        "    def __call__(self, img):\n",
        "        h, w = img.size(1), img.size(2)\n",
        "        mask = np.ones((h, w), np.float32)\n",
        "        y = np.random.randint(h)\n",
        "        x = np.random.randint(w)\n",
        "\n",
        "        y1 = np.clip(y - self.length // 2, 0, h)\n",
        "        y2 = np.clip(y + self.length // 2, 0, h)\n",
        "        x1 = np.clip(x - self.length // 2, 0, w)\n",
        "        x2 = np.clip(x + self.length // 2, 0, w)\n",
        "\n",
        "        mask[y1: y2, x1: x2] = 0.\n",
        "        mask = torch.from_numpy(mask)\n",
        "        mask = mask.expand_as(img)\n",
        "        img *= mask\n",
        "        return img\n",
        "\n",
        "\n",
        "def _data_transforms_cifar10(args):\n",
        "  CIFAR_MEAN = [0.49139968, 0.48215827, 0.44653124]\n",
        "  CIFAR_STD = [0.24703233, 0.24348505, 0.26158768]\n",
        "\n",
        "  train_transform = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
        "  ])\n",
        "  if args.cutout:\n",
        "    train_transform.transforms.append(Cutout(args.cutout_length))\n",
        "\n",
        "  valid_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(CIFAR_MEAN, CIFAR_STD),\n",
        "    ])\n",
        "  return train_transform, valid_transform\n",
        "\n",
        "\n",
        "def count_parameters_in_MB(model):\n",
        "  return np.sum(np.prod(v.size()) for name, v in model.named_parameters() if \"auxiliary\" not in name)/1e6\n",
        "\n",
        "\n",
        "def save_checkpoint(state, is_best, save):\n",
        "  filename = os.path.join(save, 'checkpoint.pth.tar')\n",
        "  torch.save(state, filename)\n",
        "  if is_best:\n",
        "    best_filename = os.path.join(save, 'model_best.pth.tar')\n",
        "    shutil.copyfile(filename, best_filename)\n",
        "\n",
        "\n",
        "def save(model, model_path):\n",
        "  torch.save(model.state_dict(), model_path)\n",
        "\n",
        "\n",
        "def load(model, model_path):\n",
        "  model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "\n",
        "def drop_path(x, drop_prob):\n",
        "  if drop_prob > 0.:\n",
        "    keep_prob = 1.-drop_prob\n",
        "    mask = Variable(torch.cuda.FloatTensor(x.size(0), 1, 1, 1).bernoulli_(keep_prob))\n",
        "    x.div_(keep_prob)\n",
        "    x.mul_(mask)\n",
        "  return x\n",
        "\n",
        "\n",
        "def create_exp_dir(path, scripts_to_save=None):\n",
        "  if not os.path.exists(path):\n",
        "    os.mkdir(path)\n",
        "  print('Experiment dir : {}'.format(path))\n",
        "\n",
        "  if scripts_to_save is not None:\n",
        "    os.mkdir(os.path.join(path, 'scripts'))\n",
        "    for script in scripts_to_save:\n",
        "      dst_file = os.path.join(path, 'scripts', os.path.basename(script))\n",
        "      shutil.copyfile(script, dst_file)\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6LEKtYylaCe"
      },
      "source": [
        "# **operations.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp7p8QwrhgWn"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "OPS = {\n",
        "  'none' : lambda C, stride, affine: Zero(stride),\n",
        "  'avg_pool_3x3' : lambda C, stride, affine: nn.AvgPool2d(3, stride=stride, padding=1, count_include_pad=False),\n",
        "  'max_pool_3x3' : lambda C, stride, affine: nn.MaxPool2d(3, stride=stride, padding=1),\n",
        "  'skip_connect' : lambda C, stride, affine: Identity() if stride == 1 else FactorizedReduce(C, C, affine=affine),\n",
        "  'sep_conv_3x3' : lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),\n",
        "  'sep_conv_5x5' : lambda C, stride, affine: SepConv(C, C, 5, stride, 2, affine=affine),\n",
        "  'sep_conv_7x7' : lambda C, stride, affine: SepConv(C, C, 7, stride, 3, affine=affine),\n",
        "  'dil_conv_3x3' : lambda C, stride, affine: DilConv(C, C, 3, stride, 2, 2, affine=affine),\n",
        "  'dil_conv_5x5' : lambda C, stride, affine: DilConv(C, C, 5, stride, 4, 2, affine=affine),\n",
        "  'conv_7x1_1x7' : lambda C, stride, affine: nn.Sequential(\n",
        "    nn.ReLU(inplace=False),\n",
        "    nn.Conv2d(C, C, (1,7), stride=(1, stride), padding=(0, 3), bias=False),\n",
        "    nn.Conv2d(C, C, (7,1), stride=(stride, 1), padding=(3, 0), bias=False),\n",
        "    nn.BatchNorm2d(C, affine=affine)\n",
        "    ),\n",
        "}\n",
        "\n",
        "class ReLUConvBN(nn.Module):\n",
        "\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    super(ReLUConvBN, self).__init__()\n",
        "    self.op = nn.Sequential(\n",
        "      nn.ReLU(inplace=False),\n",
        "      nn.Conv2d(C_in, C_out, kernel_size, stride=stride, padding=padding, bias=False),\n",
        "      nn.BatchNorm2d(C_out, affine=affine)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.op(x)\n",
        "\n",
        "class DilConv(nn.Module):\n",
        "\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n",
        "    super(DilConv, self).__init__()\n",
        "    self.op = nn.Sequential(\n",
        "      nn.ReLU(inplace=False),\n",
        "      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, dilation=dilation, groups=C_in, bias=False),\n",
        "      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n",
        "      nn.BatchNorm2d(C_out, affine=affine),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.op(x)\n",
        "\n",
        "\n",
        "class SepConv(nn.Module):\n",
        "\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    super(SepConv, self).__init__()\n",
        "    self.op = nn.Sequential(\n",
        "      nn.ReLU(inplace=False),\n",
        "      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=False),\n",
        "      nn.Conv2d(C_in, C_in, kernel_size=1, padding=0, bias=False),\n",
        "      nn.BatchNorm2d(C_in, affine=affine),\n",
        "      nn.ReLU(inplace=False),\n",
        "      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=1, padding=padding, groups=C_in, bias=False),\n",
        "      nn.Conv2d(C_in, C_out, kernel_size=1, padding=0, bias=False),\n",
        "      nn.BatchNorm2d(C_out, affine=affine),\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.op(x)\n",
        "\n",
        "\n",
        "class Identity(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(Identity, self).__init__()\n",
        "\n",
        "  def forward(self, x):\n",
        "    return x\n",
        "\n",
        "\n",
        "class Zero(nn.Module):\n",
        "\n",
        "  def __init__(self, stride):\n",
        "    super(Zero, self).__init__()\n",
        "    self.stride = stride\n",
        "\n",
        "  def forward(self, x):\n",
        "    if self.stride == 1:\n",
        "      return x.mul(0.)\n",
        "    return x[:,:,::self.stride,::self.stride].mul(0.)\n",
        "\n",
        "# conv1x1 (pointwise conv)を行うクラスと予想。\n",
        "class FactorizedReduce(nn.Module):\n",
        "  def __init__(self, C_in, C_out, affine=True):\n",
        "    super(FactorizedReduce, self).__init__()\n",
        "    assert C_out % 2 == 0\n",
        "    # assert 条件式, 条件式がFalseの場合に出力するメッセージ\n",
        "    # c_outは、CやC_curr(Cellに渡される時はCとなる)が代入されてこの関数が動く、reduction = True/False時に\n",
        "    # 2がかけられるが、基本的に変化なし。Cのlayesごとの変動は[16,16,32,32,32,64,64,64]である。\n",
        "    self.relu = nn.ReLU(inplace=False)\n",
        "    self.conv_1 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
        "    self.conv_2 = nn.Conv2d(C_in, C_out // 2, 1, stride=2, padding=0, bias=False)\n",
        "    # C_outは、2で確実に割り切れるために、c_outを2で割って問題なし。\n",
        "    # catについて\n",
        "    # https://qiita.com/Haaamaaaaa/items/709d774698082e9d342d\n",
        "    self.bn = nn.BatchNorm2d(C_out, affine=affine)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.relu(x)\n",
        "    out = torch.cat([self.conv_1(x), self.conv_2(x[:,:,1:,1:])], dim=1)\n",
        "    # x(画像trainデータ) は四次元か。batch,chanel数、H,Wか\n",
        "    # self.conv_2(x[:,:,1:,1:])は、、、\n",
        "    # xのカーネルサイズが奇数以外の時は、成り立つ。恐らく奇数にはならない。\n",
        "    out = self.bn(out)\n",
        "    return out\n",
        "    # 出力は、stride=2より、ダウンサンプリングされている。　１/２である。\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rz1yo44zlft1"
      },
      "source": [
        "# **genotypes.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krk7yVXvhpfp"
      },
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')\n",
        "\n",
        "PRIMITIVES = [\n",
        "    'none',\n",
        "    'max_pool_3x3',\n",
        "    'avg_pool_3x3',\n",
        "    'skip_connect',\n",
        "    'sep_conv_3x3',\n",
        "    'sep_conv_5x5',\n",
        "    'dil_conv_3x3',\n",
        "    'dil_conv_5x5'\n",
        "]\n",
        "\n",
        "NASNet = Genotype(\n",
        "  normal = [\n",
        "    ('sep_conv_5x5', 1),\n",
        "    ('sep_conv_3x3', 0),\n",
        "    ('sep_conv_5x5', 0),\n",
        "    ('sep_conv_3x3', 0),\n",
        "    ('avg_pool_3x3', 1),\n",
        "    ('skip_connect', 0),\n",
        "    ('avg_pool_3x3', 0),\n",
        "    ('avg_pool_3x3', 0),\n",
        "    ('sep_conv_3x3', 1),\n",
        "    ('skip_connect', 1),\n",
        "  ],\n",
        "  normal_concat = [2, 3, 4, 5, 6],\n",
        "  reduce = [\n",
        "    ('sep_conv_5x5', 1),\n",
        "    ('sep_conv_7x7', 0),\n",
        "    ('max_pool_3x3', 1),\n",
        "    ('sep_conv_7x7', 0),\n",
        "    ('avg_pool_3x3', 1),\n",
        "    ('sep_conv_5x5', 0),\n",
        "    ('skip_connect', 3),\n",
        "    ('avg_pool_3x3', 2),\n",
        "    ('sep_conv_3x3', 2),\n",
        "    ('max_pool_3x3', 1),\n",
        "  ],\n",
        "  reduce_concat = [4, 5, 6],\n",
        ")\n",
        "    \n",
        "AmoebaNet = Genotype(\n",
        "  normal = [\n",
        "    ('avg_pool_3x3', 0),\n",
        "    ('max_pool_3x3', 1),\n",
        "    ('sep_conv_3x3', 0),\n",
        "    ('sep_conv_5x5', 2),\n",
        "    ('sep_conv_3x3', 0),\n",
        "    ('avg_pool_3x3', 3),\n",
        "    ('sep_conv_3x3', 1),\n",
        "    ('skip_connect', 1),\n",
        "    ('skip_connect', 0),\n",
        "    ('avg_pool_3x3', 1),\n",
        "    ],\n",
        "  normal_concat = [4, 5, 6],\n",
        "  reduce = [\n",
        "    ('avg_pool_3x3', 0),\n",
        "    ('sep_conv_3x3', 1),\n",
        "    ('max_pool_3x3', 0),\n",
        "    ('sep_conv_7x7', 2),\n",
        "    ('sep_conv_7x7', 0),\n",
        "    ('avg_pool_3x3', 1),\n",
        "    ('max_pool_3x3', 0),\n",
        "    ('max_pool_3x3', 1),\n",
        "    ('conv_7x1_1x7', 0),\n",
        "    ('sep_conv_3x3', 5),\n",
        "  ],\n",
        "  reduce_concat = [3, 4, 6]\n",
        ")\n",
        "\n",
        "DARTS_V1 = Genotype(normal=[('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 0), ('sep_conv_3x3', 1), ('skip_connect', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('skip_connect', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 0), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('avg_pool_3x3', 0)], reduce_concat=[2, 3, 4, 5])\n",
        "DARTS_V2 = Genotype(normal=[('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 0), ('sep_conv_3x3', 1), ('sep_conv_3x3', 1), ('skip_connect', 0), ('skip_connect', 0), ('dil_conv_3x3', 2)], normal_concat=[2, 3, 4, 5], reduce=[('max_pool_3x3', 0), ('max_pool_3x3', 1), ('skip_connect', 2), ('max_pool_3x3', 1), ('max_pool_3x3', 0), ('skip_connect', 2), ('skip_connect', 2), ('max_pool_3x3', 1)], reduce_concat=[2, 3, 4, 5])\n",
        "\n",
        "DARTS = DARTS_V2\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6atx_d5In9U4"
      },
      "source": [
        "# **model_search.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qi8-mxAhh-u"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class MixedOp(nn.Module):\n",
        "\n",
        "  def __init__(self, C, stride):\n",
        "    super(MixedOp, self).__init__()\n",
        "    self._ops = nn.ModuleList()\n",
        "    # あるCellの各オペレーション処理を全て包含するリストとなる。\n",
        "\n",
        "    for primitive in PRIMITIVES:\n",
        "      op = OPS[primitive](C, stride, False)\n",
        "      # PRIMITIVES = ['none','max_pool_3x3','avg_pool_3x3','skip_connect','sep_conv_3x3','sep_conv_5x5','dil_conv_3x3','dil_conv_5x5']\n",
        "      # PRIMITIVESは、各実行名称が入っているリストである。\n",
        "      # OPSは各、名称をキーとしてlamda関数を値とする、辞書が作成されていてそれを利用することで関数を定義できる。\n",
        "      if 'pool' in primitive:\n",
        "        op = nn.Sequential(op, nn.BatchNorm2d(C, affine=False))\n",
        "      # 'max_pool_3x3','avg_pool_3x3'のop処理後に、batchNorm2dを行う。\n",
        "      # pool後のみ、batchnorm採用？\n",
        "\n",
        "      self._ops.append(op)\n",
        "      # 各、オペレーションを_opsに追加していく。\n",
        "\n",
        "      ##### ちなみに、strideとCによって、このオペレーションたちは変更し得るので毎回作成してる。 #####\n",
        "\n",
        "  def forward(self, x, weights):\n",
        "    return sum(w * op(x) for w, op in zip(weights, self._ops))\n",
        "\n",
        "\n",
        "class Cell(nn.Module):\n",
        "\n",
        "  def __init__(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
        "    super(Cell, self).__init__()\n",
        "    self.reduction = reduction\n",
        "    # 現在の削減状態、PrevがTrue時にダウンサンプリングされる。カーネルサイズが小さくなる。1/2倍\n",
        "\n",
        "    if reduction_prev:\n",
        "      self.preprocess0 = FactorizedReduce(C_prev_prev, C, affine=False)\n",
        "      # reduction_prev == True 時、実行。\n",
        "      # skip層であり、stride=2より、dawnsamplingされる。カーネルサイズが1/2になる。\n",
        "    else:\n",
        "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0, affine=False)\n",
        "      # reduction_prev == False 時、実行。\n",
        "      # (C_in, C_out, kernel_size, stride, padding, affine=True)となるただのconv層である。\n",
        "      # ただの1*1のconv層　で、stride=1よりdawnsamplingされない。\n",
        "\n",
        "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0, affine=False)\n",
        "    self._steps = steps\n",
        "    self._multiplier = multiplier\n",
        "    # Network作成時に、steps=4、multiplier=3 がデフォルトで設定されている。\n",
        "\n",
        "    # preprocessは、optを適用させる上での前過程。\n",
        "\n",
        "    self._ops = nn.ModuleList()\n",
        "    self._bns = nn.ModuleList()\n",
        "    for i in range(self._steps):\n",
        "      for j in range(2+i):\n",
        "        stride = 2 if reduction and j < 2 else 1\n",
        "        # reduction==True かつ j<2の時、stride=1となる。\n",
        "        # 12/9 07:08 何故、j が 2以上ならOKなの？ reductionがTrueの時は前処理で1/2される。\n",
        "        #  j  = 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4\n",
        "        # str = 2, 2, 2, 2, 1, 2, 2, 1, 1, 2, 2, 1, 1, 1\n",
        "        # 2個分のの入力ノードを含みます\n",
        "        # リダクションは入力ノードにのみ使用する必要があるため。\n",
        "        # [解釈] : これは論文読むべき？\n",
        "        # 理解 : 知らんかったけど、そもそもノードとセルっていう概念があって。\n",
        "        # 恐らくセルは層と同じ意味。　しかしノードを包含しているのがセルであり、各ノードごとにオペレーションの選択肢がある。\n",
        "        # そもそもstepsはノードの個数？というかノードのグラフの階差みたいなイメージだった。\n",
        "\n",
        "        op = MixedOp(C, stride)\n",
        "        # 各、Cとstride毎に、あるCellの各オペレーション処理を全て包含するリストとなる。\n",
        "        self._ops.append(op)\n",
        "        # len(_ops)14個となる。\n",
        "\n",
        "  def forward(self, s0, s1, weights):\n",
        "    s0 = self.preprocess0(s0)\n",
        "    s1 = self.preprocess1(s1)\n",
        "\n",
        "    states = [s0, s1]\n",
        "    offset = 0\n",
        "    for i in range(self._steps):\n",
        "      s = sum(self._ops[offset+j](h, weights[offset+j]) for j, h in enumerate(states))\n",
        "      offset += len(states)\n",
        "      states.append(s)\n",
        "\n",
        "    return torch.cat(states[-self._multiplier:], dim=1)\n",
        "\n",
        "\n",
        "class Network(nn.Module):\n",
        "\n",
        "  def __init__(self, C, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):\n",
        "    super(Network, self).__init__()\n",
        "\n",
        "    self._C = C\n",
        "    # C は args.init_channelsであり、入力チャンネル数:16\n",
        "\n",
        "    self._num_classes = num_classes\n",
        "    self._layers = layers\n",
        "    self._criterion = criterion\n",
        "    self._steps = steps\n",
        "    self._multiplier = multiplier\n",
        "\n",
        "    C_curr = stem_multiplier*C\n",
        "    #  48   = 3 * (C=16)\n",
        "\n",
        "    self.stem = nn.Sequential(\n",
        "      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(C_curr)\n",
        "    )\n",
        "    # 最初の層のconv層　チャンネル数が3\n",
        "    # C_currはconvの出力フィルター数 → 上記より48\n",
        "    # currentで今のフィルター出力数を表している\n",
        "\n",
        "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
        "    # C_prev_prev = (C_curr = 48)\n",
        "    # C_prev = (C_curr = 48)\n",
        "    # C_curr = (C = 16)\n",
        "    # 各初期、出力フィルター数\n",
        "\n",
        "    self.cells = nn.ModuleList()\n",
        "    # cells は cellを詰め込むリスト\n",
        "    reduction_prev = False\n",
        "    # 一つ前ののreductionである、 reduction_prevの初期は、１つ前がないからFalse\n",
        "\n",
        "    for i in range(layers):\n",
        "      if i in [layers//3, 2*layers//3]:\n",
        "        # (layesの3の商　or　2*layersの3の商)時、実行\n",
        "        # default = 8の時、i == 2 or 5 で実行される。\n",
        "        C_curr *= 2\n",
        "        reduction = True\n",
        "        # つまり、i==2と i==5の 3番目と6番目の層は、reduction = Trueとなり、フィルター数も二倍\n",
        "      else:\n",
        "        reduction = False\n",
        "\n",
        "      cell = Cell(steps, multiplier, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
        "      # Cell(self, steps, multiplier, C_prev_prev, C_prev, C, reduction, reduction_prev)で作成\n",
        "      # i==2と i==5の 3番目と6番目の層は、reductionをTrue(c_curr*2)/Falseとして、Cellを作成していく。\n",
        "\n",
        "      reduction_prev = reduction\n",
        "      # 使用したreductionは、reduction_prevとなる。\n",
        "\n",
        "      self.cells += [cell]\n",
        "      # 作成した　cell　をcellsに追加\n",
        "\n",
        "      C_prev_prev, C_prev = C_prev, multiplier*C_curr\n",
        "      # preはprev_prevとなり、prevは multiplier*C_curr = (default=4)*C_curr となる。\n",
        "\n",
        "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "    self.classifier = nn.Linear(C_prev, num_classes)\n",
        "\n",
        "    self._initialize_alphas()\n",
        "\n",
        "  def new(self):\n",
        "    model_new = Network(self._C, self._num_classes, self._layers, self._criterion).cuda()\n",
        "    for x, y in zip(model_new.arch_parameters(), self.arch_parameters()):\n",
        "        x.data.copy_(y.data)\n",
        "        # arch_parameters = self.alphas_normal, self.alphas_reduce\n",
        "        # 現状のモデルを コピーして新しく作り直している。\n",
        "    return model_new\n",
        "\n",
        "  def forward(self, input):\n",
        "    s0 = s1 = self.stem(input)\n",
        "    for i, cell in enumerate(self.cells):\n",
        "      if cell.reduction:\n",
        "        weights = F.softmax(self.alphas_reduce, dim=-1)\n",
        "      else:\n",
        "        weights = F.softmax(self.alphas_normal, dim=-1)\n",
        "      s0, s1 = s1, cell(s0, s1, weights)\n",
        "    out = self.global_pooling(s1)\n",
        "    logits = self.classifier(out.view(out.size(0),-1))\n",
        "    return logits\n",
        "\n",
        "  def _loss(self, input, target):\n",
        "    logits = self(input)\n",
        "    return self._criterion(logits, target)\n",
        "\n",
        "  def _initialize_alphas(self):\n",
        "    k = sum(1 for i in range(self._steps) for n in range(2+i))\n",
        "    num_ops = len(PRIMITIVES)\n",
        "\n",
        "    # kは各ノードの矢印の総本数\n",
        "\n",
        "    # torch.autograd.variable(torch.tensor,requires_grad=True/False)によって「αnormal」と「αreduce」を勾配対象\n",
        "    self.alphas_normal = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\n",
        "    self.alphas_reduce = Variable(1e-3*torch.randn(k, num_ops).cuda(), requires_grad=True)\n",
        "    self._arch_parameters = [\n",
        "      self.alphas_normal,\n",
        "      self.alphas_reduce,\n",
        "    ]\n",
        "    # torch.randn(k, num_ops)は、{k × num_ops}のsizeとなるランダム値を生成。\n",
        "    # 矢印の本数 × オペレーションの数\n",
        "    # それぞれの矢印のオペレーションに適した、重みとなる。\n",
        "\n",
        "  def arch_parameters(self):\n",
        "    return self._arch_parameters\n",
        "\n",
        "  def genotype(self):\n",
        "\n",
        "    def _parse(weights):\n",
        "      gene = []\n",
        "      n = 2\n",
        "      start = 0\n",
        "      for i in range(self._steps):\n",
        "        end = start + n\n",
        "        W = weights[start:end].copy()\n",
        "        # 1回目： start = 0 end = 2 n = 2 edge は0 ~ 1 \n",
        "        # 2回目： start = 2 end = 5 n = 3 edge は2 ~ 4 \n",
        "        # 3回目： start = 5 end = 9 n = 4 edge は5 ~ 8 \n",
        "        # 4回目： start = 9 end = 14 n = 5 edge は9 ~ 13 \n",
        "        edges = sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none')))[:2]\n",
        "        # sortのkeyが、14本のedgeから、それぞれ最大値のオペレーションが比較基準となる。\n",
        "        # 各step毎に2本のedgeが選択されている。\n",
        "        # stepsが進むにつれて(forによって)、rangeの範囲が一つずつ増えていく。steps = 4より　最大で0 ~ 4であり、14本見てなくない？\n",
        "\n",
        "        # print(sorted(range(i + 2), key=lambda x: -max(W[x][k] for k in range(len(W[x])) if k != PRIMITIVES.index('none'))))\n",
        "        # print(edges)\n",
        "\n",
        "        for j in edges:\n",
        "          k_best = None\n",
        "          for k in range(len(W[j])):\n",
        "            # 選ばれたedgeの各オペレーションを見ていく。\n",
        "            if k != PRIMITIVES.index('none'):\n",
        "              if k_best is None or W[j][k] > W[j][k_best]:\n",
        "                k_best = k\n",
        "              # 選んだedge内での最大のオペレーションを選んでいく。\n",
        "          gene.append((PRIMITIVES[k_best], j))\n",
        "          # PRIMITIVES[k_best]は選んだedgeの最大オペレーション、jはedge番号。\n",
        "        start = end\n",
        "        n += 1\n",
        "      return gene\n",
        "\n",
        "    gene_normal = _parse(F.softmax(self.alphas_normal, dim=-1).data.cpu().numpy())\n",
        "    # print(\"----------------------------------------  gane_一覧 ----------------------------------------\")\n",
        "    # print(F.softmax(self.alphas_normal, dim=-1))\n",
        "    # print(*gene_normal)\n",
        "    # print(\"---------------------------------------- gane_一覧---------------------------------------- \")\n",
        "    gene_reduce = _parse(F.softmax(self.alphas_reduce, dim=-1).data.cpu().numpy())\n",
        "\n",
        "    concat = range(2+self._steps-self._multiplier, self._steps+2)\n",
        "    genotype = Genotype(\n",
        "      normal=gene_normal, normal_concat=concat,\n",
        "      reduce=gene_reduce, reduce_concat=concat\n",
        "    )\n",
        "    return genotype\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-WUeWVDoPmHC"
      },
      "source": [
        "# model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
        "# model.genotype()"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNh9C5r0lj8e"
      },
      "source": [
        "# **architect.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLJxtkFuhZIH"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def _concat(xs):\n",
        "  return torch.cat([x.view(-1) for x in xs])\n",
        "\n",
        "\n",
        "class Architect(object):\n",
        "\n",
        "  def __init__(self, model, args):\n",
        "    self.network_momentum = args.momentum\n",
        "    # momentum = 0.9がdefault\n",
        "    self.network_weight_decay = args.weight_decay\n",
        "    # weight_decay = 3e-4 がdefault\n",
        "    # 学習用の値を扱うため、trainで活用されそう。\n",
        "    self.model = model\n",
        "    self.optimizer = torch.optim.Adam(self.model.arch_parameters(),\n",
        "        lr=args.arch_learning_rate, betas=(0.5, 0.999), weight_decay=args.arch_weight_decay)\n",
        "\n",
        "  # unrolled_model を 計算する関数？\n",
        "  def _compute_unrolled_model(self, input, target, eta, network_optimizer):\n",
        "    # https://qiita.com/tokkuman/items/1944c00415d129ca0ee9\n",
        "    ########## モーメント と wight_decayを計算するよ。 ###########\n",
        "    loss = self.model._loss(input, target)\n",
        "    theta = _concat(self.model.parameters()).data\n",
        "    # torch.cat([x.view(-1) for x in xs])\n",
        "    # 重みパラメータを結合し続ける。\n",
        "    try:\n",
        "      moment = _concat(network_optimizer.state[v]['momentum_buffer'] for v in self.model.parameters()).mul_(self.network_momentum)\n",
        "      # optimizer.state の self.model.parameters()(パラメータ名) を キーとして、'momentum_buffer' のキーを更に選択して\n",
        "      # 一列に並べてつなげる。\n",
        "      # その後.mul_で　self.network_momentum　を掛け合わせる。\n",
        "      # αΔwt　←モーメントで、それを求めている。\n",
        "      # Δwt = optimizer.state[各パラメータ]['momentum_buffer']、　α = network_momentumである。\n",
        "\n",
        "    except:\n",
        "      moment = torch.zeros_like(theta)\n",
        "      # thetaと同等のsizeの 0 テンソルを作成する。\n",
        "\n",
        "    dtheta = _concat(torch.autograd.grad(loss, self.model.parameters())).data + self.network_weight_decay*theta\n",
        "    #　weight_decay : ηλw\n",
        "    # λ = network_weight_decay\n",
        "    # W = theta (重みパラメータの一列)\n",
        "    # δL/δw が　左。\n",
        "\n",
        "    # dtheta = δL/δw + λw\n",
        "    # moment = Δwt\n",
        "\n",
        "    # 最終式 : w ← w -η(δL/δw) - ηλw + αΔwt\n",
        "\n",
        "    unrolled_model = self._construct_model_from_theta(theta.sub(eta, moment+dtheta))\n",
        "    # torch.sub(a,b,c=1)\n",
        "    # moment + dtheta　= Δwt - δL/δw - λw\n",
        "    # a.sub(other,alpha) a - othre * alpha\n",
        "    # w - eta(η) * (Δwt + δL/δw + λw)\n",
        "\n",
        "    # unrolled_modelは、最終的に、現存モデル　から　新しい複製モデルに、パラメータを移行した状態となる。\n",
        "    return unrolled_model\n",
        "\n",
        "  def step(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer, unrolled):\n",
        "    # eta は学習率\n",
        "    self.optimizer.zero_grad()\n",
        "    if unrolled:\n",
        "        # unrolled の　default は　Falseである。\n",
        "        self._backward_step_unrolled(input_train, target_train, input_valid, target_valid, eta, network_optimizer)\n",
        "    else:\n",
        "        self._backward_step(input_valid, target_valid)\n",
        "        # lossを計算して、backwardするだけ。\n",
        "    self.optimizer.step()\n",
        "\n",
        "  def _backward_step(self, input_valid, target_valid):\n",
        "    loss = self.model._loss(input_valid, target_valid)\n",
        "    loss.backward()\n",
        "\n",
        "  def _backward_step_unrolled(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer):\n",
        "    unrolled_model = self._compute_unrolled_model(input_train, target_train, eta, network_optimizer)\n",
        "    unrolled_loss = unrolled_model._loss(input_valid, target_valid)\n",
        "\n",
        "    unrolled_loss.backward()\n",
        "    dalpha = [v.grad for v in unrolled_model.arch_parameters()]\n",
        "    vector = [v.grad.data for v in unrolled_model.parameters()]\n",
        "    # arch_parameters = self.alphas_normal, self.alphas_reduce\n",
        "    # arch_parameters　を　vに入れて、grad →　tensor([6., 5.])みたいなテンソルで返す。\n",
        "    # ちなみに、grad.data と grad の違いがわからない。\n",
        "\n",
        "    implicit_grads = self._hessian_vector_product(vector, input_train, target_train)\n",
        "    # (gradベクトル,input_train,target_train)\n",
        "\n",
        "    for g, ig in zip(dalpha, implicit_grads):\n",
        "      g.data.sub_(eta, ig.data)\n",
        "\n",
        "    for v, g in zip(self.model.arch_parameters(), dalpha):\n",
        "      if v.grad is None:\n",
        "        v.grad = Variable(g.data)\n",
        "      else:\n",
        "        v.grad.data.copy_(g.data)\n",
        "\n",
        "  def _construct_model_from_theta(self, theta):\n",
        "    model_new = self.model.new()\n",
        "    # 現存モデルをコピー\n",
        "    model_dict = self.model.state_dict()\n",
        "    # 現存モデルのstate_dict(conv1とかチャネル1の重みパラメータを取り出す)\n",
        "\n",
        "    # theta : w - eta(η) * (Δwt + δL/δw + λw)\n",
        "\n",
        "    params, offset = {}, 0\n",
        "    for k, v in self.model.named_parameters():\n",
        "      v_length = np.prod(v.size())\n",
        "      # v.size()の要素積となる。 vの総要素数\n",
        "\n",
        "      params[k] = theta[offset: offset+v_length].view(v.size())\n",
        "      # それぞれの勾配式、適用後の要素を一列化\n",
        "      # params = [(0 : v_length), (v_length : 2 * v_length), (2 * v_length : 3 * v_length),,,]\n",
        "      # 各forwardの1操作の重みパラメータが取り出される。\n",
        "\n",
        "      offset += v_length\n",
        "\n",
        "    assert offset == len(theta)\n",
        "    model_dict.update(params)\n",
        "    model_new.load_state_dict(model_dict)\n",
        "    return model_new.cuda()\n",
        "\n",
        "  def _hessian_vector_product(self, vector, input, target, r=1e-2):\n",
        "    # ヘッセ行列の関数ー数値微分の近似部分？  正解\n",
        "    # (gradベクトル,input_train,target_train)\n",
        "    R = r / _concat(vector).norm()\n",
        "    # grad ベクトル一列 を 2ノルム化したもので r を割る。\n",
        "    for p, v in zip(self.model.parameters(), vector):\n",
        "      p.data.add_(R, v)\n",
        "    loss = self.model._loss(input, target)\n",
        "    grads_p = torch.autograd.grad(loss, self.model.arch_parameters())\n",
        "\n",
        "    for p, v in zip(self.model.parameters(), vector):\n",
        "      p.data.sub_(2*R, v)\n",
        "    loss = self.model._loss(input, target)\n",
        "    grads_n = torch.autograd.grad(loss, self.model.arch_parameters())\n",
        "\n",
        "    for p, v in zip(self.model.parameters(), vector):\n",
        "      p.data.add_(R, v)\n",
        "\n",
        "    return [(x-y).div_(2*R) for x, y in zip(grads_p, grads_n)]\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyX2msH4ozNo"
      },
      "source": [
        "\n",
        "# **train_search.pyの前半**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q-yWTRfQovuN"
      },
      "source": [
        "def train(train_queue, valid_queue, model, architect, criterion, optimizer, lr):\n",
        "  objs = AvgrageMeter()\n",
        "  top1 = AvgrageMeter()\n",
        "  top5 = AvgrageMeter()\n",
        "  #len(train_queue) 1563\n",
        "\n",
        "  for step, (input, target) in enumerate(train_queue):\n",
        "    # !nvidia-smi\n",
        "    model.train()\n",
        "    n = input.size(0)\n",
        "\n",
        "    input = Variable(input, requires_grad=False).cuda()\n",
        "    # target = Variable(target, requires_grad=False).cuda(async=True)\n",
        "    target = Variable(target, requires_grad=False).cuda(non_blocking=True)\n",
        "    # Variable  Tensorをラップしたクラス。\n",
        "    # data grad creatorを保有するクラス。\n",
        "\n",
        "    input_search, target_search = next(iter(valid_queue))\n",
        "    # iter(リスト)でイテレータを作成。\n",
        "    #nextで、イテレータを一つずつ取り出す。\n",
        "\n",
        "    input_search = Variable(input_search, requires_grad=False).cuda()\n",
        "    # target_search = Variable(target_search, requires_grad=False).cuda(async=True)\n",
        "    target_search = Variable(target_search, requires_grad=False).cuda(non_blocking=True)\n",
        "\n",
        "    architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)\n",
        "    # step(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer, unrolled)\n",
        "    # unrolled は　(default=False)であり、展開するかしないの意味とは？\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits = model(input)\n",
        "    loss = criterion(logits, target)\n",
        "\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "\n",
        "    objs.update(loss.data, n)\n",
        "    top1.update(prec1.data, n)\n",
        "    top5.update(prec5.data, n)\n",
        "\n",
        "    if step % args.report_freq == 0:\n",
        "      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
        "      # summary(model,(3,32,32))\n",
        "  \n",
        "  return top1.avg, objs.avg\n",
        "\n",
        "\n",
        "def infer(valid_queue, model, criterion):\n",
        "  objs = AvgrageMeter()\n",
        "  top1 = AvgrageMeter()\n",
        "  top5 = AvgrageMeter()\n",
        "  # summary(model,(3,32,32))\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    for step, (input, target) in enumerate(valid_queue):\n",
        "      input = Variable(input, volatile=True).cuda()\n",
        "      target = Variable(target, volatile=True).cuda(async=True)\n",
        "\n",
        "      logits = model(input)\n",
        "      loss = criterion(logits, target)\n",
        "\n",
        "      prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "      n = input.size(0)\n",
        "      objs.update(loss.data, n)\n",
        "      top1.update(prec1.data, n)\n",
        "      top5.update(prec5.data, n)\n",
        "\n",
        "      if step % args.report_freq == 0:\n",
        "        logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
        "\n",
        "  return top1.avg, objs.avg"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4wQsFnblpK4"
      },
      "source": [
        "# **train_search.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-a06XfEndhig",
        "outputId": "eb89ad69-5e66-4395-af89-59e69b27ffad"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "import torch.utils\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as dset\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.utils\n",
        "\n",
        "from torch.autograd import Variable\n",
        "# from model_search import Network\n",
        "# from architect import Architect\n",
        "\n",
        "parser = argparse.ArgumentParser(\"cifar\")\n",
        "parser.add_argument('--data', type=str, default='./data', help='location of the data corpus')\n",
        "# parser.add_argument('--batch_size', type=int, default=64, help='batch size')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
        "parser.add_argument('--learning_rate_min', type=float, default=0.001, help='min learning rate')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
        "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
        "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
        "parser.add_argument('--epochs', type=int, default=1, help='num of training epochs')\n",
        "parser.add_argument('--init_channels', type=int, default=16, help='num of init channels')\n",
        "parser.add_argument('--layers', type=int, default=8, help='total number of layers')\n",
        "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
        "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
        "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
        "parser.add_argument('--drop_path_prob', type=float, default=0.3, help='drop path probability')\n",
        "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
        "parser.add_argument('--seed', type=int, default=2, help='random seed')\n",
        "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
        "parser.add_argument('--train_portion', type=float, default=0.5, help='portion of training data')\n",
        "parser.add_argument('--unrolled', action='store_true', default=False, help='use one-step unrolled validation loss')\n",
        "parser.add_argument('--arch_learning_rate', type=float, default=3e-4, help='learning rate for arch encoding')\n",
        "parser.add_argument('--arch_weight_decay', type=float, default=1e-3, help='weight decay for arch encoding')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "args.save = 'search-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
        "log_format = '%(asctime)s %(message)s'\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
        "fh.setFormatter(logging.Formatter(log_format))\n",
        "logging.getLogger().addHandler(fh)\n",
        "\n",
        "CIFAR_CLASSES = 10\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  logging.info('no gpu device available')\n",
        "  sys.exit(1)\n",
        "np.random.seed(args.seed)\n",
        "torch.cuda.set_device(args.gpu)\n",
        "cudnn.benchmark = True\n",
        "torch.manual_seed(args.seed)\n",
        "cudnn.enabled=True\n",
        "torch.cuda.manual_seed(args.seed)\n",
        "logging.info('gpu device = %d' % args.gpu)\n",
        "logging.info(\"args = %s\", args)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion = criterion.cuda()\n",
        "\n",
        "model = Network(args.init_channels, CIFAR_CLASSES, args.layers, criterion)\n",
        "# model.search.py の Network の初期引数(self, C, num_classes, layers, criterion, steps=4, multiplier=4, stem_multiplier=3):\n",
        "\n",
        "model = model.cuda()\n",
        "logging.info(\"param size = %fMB\", count_parameters_in_MB(model))\n",
        "\n",
        "optimizer = torch.optim.SGD(\n",
        "    model.parameters(),\n",
        "    args.learning_rate,\n",
        "    momentum=args.momentum,\n",
        "    weight_decay=args.weight_decay)\n",
        "\n",
        "train_transform, valid_transform = _data_transforms_cifar10(args)\n",
        "train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
        "\n",
        "num_train = len(train_data)\n",
        "indices = list(range(num_train))\n",
        "split = int(np.floor(args.train_portion * num_train))\n",
        "# split = args.train_portion(default=0.5) * num_train(len(train_data)) 30000\n",
        "# 30000 train　30000 validationをしている。\n",
        "\n",
        "train_queue = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=args.batch_size,\n",
        "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[:split]),\n",
        "    pin_memory=True, num_workers=2)\n",
        "\n",
        "valid_queue = torch.utils.data.DataLoader(\n",
        "    train_data, batch_size=args.batch_size,\n",
        "    sampler=torch.utils.data.sampler.SubsetRandomSampler(indices[split:num_train]),\n",
        "    pin_memory=True, num_workers=2)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
        "      optimizer, float(args.epochs), eta_min=args.learning_rate_min)\n",
        "# scheduler は、 args.epochs ごとに args.learning_rate_min まで下げる。下記のurlみればわかりやすいです。\n",
        "# https://katsura-jp.hatenablog.com/entry/2019/01/30/183501#PyTorch%E3%83%A9%E3%82%A4%E3%83%96%E3%83%A9%E3%83%AA%E5%86%85%E3%81%AB%E3%81%82%E3%82%8Bscheduler\n",
        "\n",
        "architect = Architect(model, args)\n",
        "# architect.pyの Architect Class\n",
        "#初期引数は(self, model, args)\n",
        "# ~疑~　12/8 04:07 architectは何をするもの？\n",
        "\n",
        "for epoch in range(args.epochs):\n",
        "  scheduler.step()\n",
        "  # epoch ごとに、scheduler.step()を実行する。\n",
        "  lr = scheduler.get_lr()[0]\n",
        "  logging.info('epoch %d lr %e', epoch, lr)\n",
        "\n",
        "  genotype = model.genotype()\n",
        "  # modelのgenotypeを目視、恐らく現在の構築されたモデル構造が明記\n",
        "  logging.info('genotype = %s', genotype)\n",
        "\n",
        "  print(F.softmax(model.alphas_normal, dim=-1))\n",
        "  print(F.softmax(model.alphas_reduce, dim=-1))\n",
        "  # modelの alphas_normal、 alphas_reduce　option の重み和となる項目のソフトマックス値を出力\n",
        "\n",
        "  # training\n",
        "  train_acc, train_obj = train(train_queue, valid_queue, model, architect, criterion, optimizer, lr)\n",
        "  # !nvidia-smi\n",
        "  # 下記のtrain関数を実行\n",
        "  logging.info('train_acc %f', train_acc)\n",
        "\n",
        "  # validation\n",
        "  valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
        "  # 下記のinfer関数を実行\n",
        "  logging.info('valid_acc %f', valid_acc)\n",
        "\n",
        "  # 各epochごとに構造が変化するので保存？\n",
        "  save(model, os.path.join(args.save, 'weights.pt'))\n",
        "\n",
        "\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment dir : search-EXP-20210108-122419\n",
            "01/08 12:24:19 PM gpu device = 0\n",
            "01/08 12:24:19 PM args = Namespace(arch_learning_rate=0.0003, arch_weight_decay=0.001, batch_size=32, cutout=False, cutout_length=16, data='./data', drop_path_prob=0.3, epochs=1, gpu=0, grad_clip=5, init_channels=16, layers=8, learning_rate=0.025, learning_rate_min=0.001, model_path='saved_models', momentum=0.9, report_freq=50, save='search-EXP-20210108-122419', seed=2, train_portion=0.5, unrolled=False, weight_decay=0.0003)\n",
            "01/08 12:24:19 PM param size = 1.930618MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "01/08 12:24:20 PM epoch 0 lr 1.000000e-03\n",
            "01/08 12:24:20 PM genotype = Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 2), ('dil_conv_5x5', 1), ('avg_pool_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_3x3', 0), ('max_pool_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_3x3', 4), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))\n",
            "tensor([[0.1250, 0.1247, 0.1251, 0.1250, 0.1251, 0.1251, 0.1250, 0.1250],\n",
            "        [0.1249, 0.1250, 0.1250, 0.1249, 0.1249, 0.1251, 0.1251, 0.1251],\n",
            "        [0.1251, 0.1250, 0.1250, 0.1251, 0.1248, 0.1251, 0.1248, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1251, 0.1249, 0.1249, 0.1250, 0.1250, 0.1251],\n",
            "        [0.1248, 0.1250, 0.1249, 0.1249, 0.1249, 0.1251, 0.1252, 0.1251],\n",
            "        [0.1249, 0.1250, 0.1249, 0.1250, 0.1251, 0.1251, 0.1250, 0.1250],\n",
            "        [0.1249, 0.1249, 0.1252, 0.1251, 0.1250, 0.1250, 0.1250, 0.1249],\n",
            "        [0.1250, 0.1248, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1251, 0.1249],\n",
            "        [0.1249, 0.1249, 0.1249, 0.1250, 0.1249, 0.1250, 0.1253, 0.1251],\n",
            "        [0.1249, 0.1251, 0.1251, 0.1250, 0.1249, 0.1249, 0.1249, 0.1251],\n",
            "        [0.1250, 0.1250, 0.1252, 0.1251, 0.1249, 0.1249, 0.1250, 0.1249],\n",
            "        [0.1249, 0.1253, 0.1250, 0.1248, 0.1248, 0.1251, 0.1251, 0.1250],\n",
            "        [0.1249, 0.1251, 0.1251, 0.1252, 0.1250, 0.1248, 0.1250, 0.1249]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n",
            "tensor([[0.1251, 0.1250, 0.1250, 0.1250, 0.1249, 0.1251, 0.1249, 0.1250],\n",
            "        [0.1249, 0.1248, 0.1252, 0.1247, 0.1251, 0.1249, 0.1252, 0.1251],\n",
            "        [0.1249, 0.1249, 0.1251, 0.1250, 0.1248, 0.1250, 0.1249, 0.1254],\n",
            "        [0.1251, 0.1250, 0.1250, 0.1250, 0.1252, 0.1250, 0.1249, 0.1250],\n",
            "        [0.1251, 0.1249, 0.1250, 0.1250, 0.1251, 0.1249, 0.1250, 0.1251],\n",
            "        [0.1251, 0.1252, 0.1251, 0.1247, 0.1252, 0.1249, 0.1249, 0.1250],\n",
            "        [0.1251, 0.1251, 0.1251, 0.1250, 0.1247, 0.1250, 0.1251, 0.1250],\n",
            "        [0.1250, 0.1249, 0.1251, 0.1250, 0.1250, 0.1248, 0.1251, 0.1248],\n",
            "        [0.1252, 0.1251, 0.1250, 0.1250, 0.1249, 0.1249, 0.1250, 0.1250],\n",
            "        [0.1248, 0.1249, 0.1250, 0.1249, 0.1252, 0.1250, 0.1251, 0.1251],\n",
            "        [0.1252, 0.1249, 0.1250, 0.1250, 0.1250, 0.1249, 0.1250, 0.1251],\n",
            "        [0.1249, 0.1251, 0.1250, 0.1250, 0.1250, 0.1251, 0.1250, 0.1249],\n",
            "        [0.1249, 0.1249, 0.1251, 0.1251, 0.1246, 0.1251, 0.1251, 0.1251],\n",
            "        [0.1250, 0.1247, 0.1250, 0.1251, 0.1252, 0.1250, 0.1250, 0.1250]],\n",
            "       device='cuda:0', grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:24:22 PM train 000 2.360637e+00 9.375000 53.125000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:25:38 PM train 050 2.195386e+00 18.995098 66.544121\n",
            "01/08 12:26:53 PM train 100 2.093620e+00 21.905941 73.452972\n",
            "01/08 12:28:09 PM train 150 2.008966e+00 25.062086 77.255798\n",
            "01/08 12:29:25 PM train 200 1.947019e+00 27.114428 79.430969\n",
            "01/08 12:30:39 PM train 250 1.902250e+00 28.847113 80.702194\n",
            "01/08 12:31:54 PM train 300 1.864524e+00 30.409052 81.883301\n",
            "01/08 12:33:07 PM train 350 1.830649e+00 31.677351 82.950500\n",
            "01/08 12:34:22 PM train 400 1.805041e+00 32.567020 83.806114\n",
            "01/08 12:35:37 PM train 450 1.781531e+00 33.508869 84.381927\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:36:51 PM train 500 1.760049e+00 34.593315 84.986275\n",
            "01/08 12:38:06 PM train 550 1.740980e+00 35.299454 85.531990\n",
            "01/08 12:39:22 PM train 600 1.722062e+00 36.137688 85.914101\n",
            "01/08 12:40:37 PM train 650 1.705400e+00 36.770351 86.367126\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:41:52 PM train 700 1.687140e+00 37.388554 86.871437\n",
            "01/08 12:43:07 PM train 750 1.673930e+00 37.812084 87.204559\n",
            "01/08 12:43:53 PM train_acc 38.124001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:59: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:60: UserWarning: This overload of cuda is deprecated:\n",
            "\tcuda(torch.device device, bool async, *, torch.memory_format memory_format)\n",
            "Consider using one of the following signatures instead:\n",
            "\tcuda(torch.device device, bool non_blocking, *, torch.memory_format memory_format) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:882.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:43:54 PM valid 000 1.763657e+00 37.500000 81.250000\n",
            "01/08 12:44:02 PM valid 050 1.440385e+00 47.671570 91.911766\n",
            "01/08 12:44:11 PM valid 100 1.424297e+00 48.483910 92.172028\n",
            "01/08 12:44:20 PM valid 150 1.436282e+00 47.806290 91.928810\n",
            "01/08 12:44:28 PM valid 200 1.434706e+00 47.932213 92.024254\n",
            "01/08 12:44:37 PM valid 250 1.438071e+00 47.734066 92.131477\n",
            "01/08 12:44:45 PM valid 300 1.434340e+00 47.996262 92.296509\n",
            "01/08 12:44:54 PM valid 350 1.431325e+00 48.112534 92.200851\n",
            "01/08 12:45:03 PM valid 400 1.427002e+00 48.316711 92.269333\n",
            "01/08 12:45:12 PM valid 450 1.429189e+00 48.156872 92.308754\n",
            "01/08 12:45:21 PM valid 500 1.432006e+00 48.128742 92.252991\n",
            "01/08 12:45:30 PM valid 550 1.431860e+00 48.145416 92.235710\n",
            "01/08 12:45:39 PM valid 600 1.431238e+00 48.018925 92.294090\n",
            "01/08 12:45:48 PM valid 650 1.427326e+00 48.094276 92.343513\n",
            "01/08 12:45:57 PM valid 700 1.427020e+00 47.953815 92.376961\n",
            "01/08 12:46:06 PM valid 750 1.427519e+00 48.015144 92.347702\n",
            "01/08 12:46:11 PM valid_acc 47.987999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cT3x6JUejVB",
        "outputId": "ba3a4432-dfb8-4611-9225-cfbbc687606d"
      },
      "source": [
        "genotype"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 2), ('dil_conv_5x5', 1), ('avg_pool_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_3x3', 0), ('max_pool_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_3x3', 4), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hne_HhA6CKZ"
      },
      "source": [
        "# **visualize.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPKpO-xS6B8C"
      },
      "source": [
        "import sys\n",
        "# import genotypes\n",
        "from graphviz import Digraph\n",
        "\n",
        "\n",
        "def plot(genotype, filename):\n",
        "  g = Digraph(\n",
        "      format='pdf',\n",
        "      edge_attr=dict(fontsize='20', fontname=\"times\"),\n",
        "      node_attr=dict(style='filled', shape='rect', align='center', fontsize='20', height='0.5', width='0.5', penwidth='2', fontname=\"times\"),\n",
        "      engine='dot')\n",
        "  g.body.extend(['rankdir=LR'])\n",
        "\n",
        "  g.node(\"c_{k-2}\", fillcolor='darkseagreen2')\n",
        "  g.node(\"c_{k-1}\", fillcolor='darkseagreen2')\n",
        "  assert len(genotype) % 2 == 0\n",
        "  steps = len(genotype) // 2\n",
        "\n",
        "  for i in range(steps):\n",
        "    g.node(str(i), fillcolor='lightblue')\n",
        "\n",
        "  for i in range(steps):\n",
        "    for k in [2*i, 2*i + 1]:\n",
        "      op, j = genotype[k]\n",
        "      if j == 0:\n",
        "        u = \"c_{k-2}\"\n",
        "      elif j == 1:\n",
        "        u = \"c_{k-1}\"\n",
        "      else:\n",
        "        u = str(j-2)\n",
        "      v = str(i)\n",
        "      g.edge(u, v, label=op, fillcolor=\"gray\")\n",
        "\n",
        "  g.node(\"c_{k}\", fillcolor='palegoldenrod')\n",
        "  for i in range(steps):\n",
        "    g.edge(str(i), \"c_{k}\", fillcolor=\"gray\")\n",
        "\n",
        "  g.render(filename, view=True)\n",
        "  g.view()\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#   if len(sys.argv) != 2:\n",
        "#     print(\"usage:\\n python {} ARCH_NAME\".format(sys.argv[0]))\n",
        "#     sys.exit(1)\n",
        "\n",
        "#   genotype_name = sys.argv[1]\n",
        "#   try:\n",
        "#     genotype = eval(\"%s\" % genotype_name)\n",
        "#   except AttributeError:\n",
        "#     print(\"{} is not specified in genotypes.py\".format(genotype_name)) \n",
        "#     sys.exit(1)\n",
        "\n",
        "#   plot(genotype.normal, \"normal\")\n",
        "#   plot(genotype.reduce, \"reduction\")\n",
        "\n",
        "plot(genotype.normal, \"normal\")\n",
        "plot(genotype.reduce, \"reduce\")\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgsuu7JQWhO4"
      },
      "source": [
        "# **model.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IHiVHAesWhYM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "class Cell(nn.Module):\n",
        "\n",
        "  def __init__(self, genotype, C_prev_prev, C_prev, C, reduction, reduction_prev):\n",
        "    super(Cell, self).__init__()\n",
        "    print(C_prev_prev, C_prev, C)\n",
        "\n",
        "    if reduction_prev:\n",
        "      self.preprocess0 = FactorizedReduce(C_prev_prev, C)\n",
        "    else:\n",
        "      self.preprocess0 = ReLUConvBN(C_prev_prev, C, 1, 1, 0)\n",
        "    self.preprocess1 = ReLUConvBN(C_prev, C, 1, 1, 0)\n",
        "    \n",
        "    if reduction:\n",
        "      op_names, indices = zip(*genotype.reduce)\n",
        "      concat = genotype.reduce_concat\n",
        "    else:\n",
        "      op_names, indices = zip(*genotype.normal)\n",
        "      concat = genotype.normal_concat\n",
        "    self._compile(C, op_names, indices, concat, reduction)\n",
        "\n",
        "  def _compile(self, C, op_names, indices, concat, reduction):\n",
        "    assert len(op_names) == len(indices)\n",
        "    self._steps = len(op_names) // 2\n",
        "    self._concat = concat\n",
        "    self.multiplier = len(concat)\n",
        "\n",
        "    self._ops = nn.ModuleList()\n",
        "    for name, index in zip(op_names, indices):\n",
        "      stride = 2 if reduction and index < 2 else 1\n",
        "      op = OPS[name](C, stride, True)\n",
        "      self._ops += [op]\n",
        "    self._indices = indices\n",
        "\n",
        "  def forward(self, s0, s1, drop_prob):\n",
        "    s0 = self.preprocess0(s0)\n",
        "    s1 = self.preprocess1(s1)\n",
        "\n",
        "    states = [s0, s1]\n",
        "    for i in range(self._steps):\n",
        "      h1 = states[self._indices[2*i]]\n",
        "      h2 = states[self._indices[2*i+1]]\n",
        "      op1 = self._ops[2*i]\n",
        "      op2 = self._ops[2*i+1]\n",
        "      h1 = op1(h1)\n",
        "      h2 = op2(h2)\n",
        "      if self.training and drop_prob > 0.:\n",
        "        if not isinstance(op1, Identity):\n",
        "          h1 = drop_path(h1, drop_prob)\n",
        "        if not isinstance(op2, Identity):\n",
        "          h2 = drop_path(h2, drop_prob)\n",
        "      s = h1 + h2\n",
        "      states += [s]\n",
        "    return torch.cat([states[i] for i in self._concat], dim=1)\n",
        "\n",
        "\n",
        "class AuxiliaryHeadCIFAR(nn.Module):\n",
        "\n",
        "  def __init__(self, C, num_classes):\n",
        "    \"\"\"assuming input size 8x8\"\"\"\n",
        "    super(AuxiliaryHeadCIFAR, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.AvgPool2d(5, stride=3, padding=0, count_include_pad=False), # image size = 2 x 2\n",
        "      nn.Conv2d(C, 128, 1, bias=False),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(128, 768, 2, bias=False),\n",
        "      nn.BatchNorm2d(768),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x.view(x.size(0),-1))\n",
        "    return x\n",
        "\n",
        "\n",
        "class AuxiliaryHeadImageNet(nn.Module):\n",
        "\n",
        "  def __init__(self, C, num_classes):\n",
        "    \"\"\"assuming input size 14x14\"\"\"\n",
        "    super(AuxiliaryHeadImageNet, self).__init__()\n",
        "    self.features = nn.Sequential(\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.AvgPool2d(5, stride=2, padding=0, count_include_pad=False),\n",
        "      nn.Conv2d(C, 128, 1, bias=False),\n",
        "      nn.BatchNorm2d(128),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(128, 768, 2, bias=False),\n",
        "      # NOTE: This batchnorm was omitted in my earlier implementation due to a typo.\n",
        "      # Commenting it out for consistency with the experiments in the paper.\n",
        "      # nn.BatchNorm2d(768),\n",
        "      nn.ReLU(inplace=True)\n",
        "    )\n",
        "    self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.features(x)\n",
        "    x = self.classifier(x.view(x.size(0),-1))\n",
        "    return x\n",
        "\n",
        "\n",
        "class NetworkCIFAR(nn.Module):\n",
        "\n",
        "  def __init__(self, C, num_classes, layers, auxiliary, genotype):\n",
        "    super(NetworkCIFAR, self).__init__()\n",
        "    self._layers = layers\n",
        "    self._auxiliary = auxiliary\n",
        "\n",
        "    stem_multiplier = 3\n",
        "    C_curr = stem_multiplier*C\n",
        "    self.stem = nn.Sequential(\n",
        "      nn.Conv2d(3, C_curr, 3, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(C_curr)\n",
        "    )\n",
        "    \n",
        "    C_prev_prev, C_prev, C_curr = C_curr, C_curr, C\n",
        "    self.cells = nn.ModuleList()\n",
        "    reduction_prev = False\n",
        "    for i in range(layers):\n",
        "      if i in [layers//3, 2*layers//3]:\n",
        "        C_curr *= 2\n",
        "        reduction = True\n",
        "      else:\n",
        "        reduction = False\n",
        "      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
        "      reduction_prev = reduction\n",
        "      self.cells += [cell]\n",
        "      C_prev_prev, C_prev = C_prev, cell.multiplier*C_curr\n",
        "      if i == 2*layers//3:\n",
        "        C_to_auxiliary = C_prev\n",
        "\n",
        "    if auxiliary:\n",
        "      self.auxiliary_head = AuxiliaryHeadCIFAR(C_to_auxiliary, num_classes)\n",
        "    self.global_pooling = nn.AdaptiveAvgPool2d(1)\n",
        "    self.classifier = nn.Linear(C_prev, num_classes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    logits_aux = None\n",
        "    s0 = s1 = self.stem(input)\n",
        "    for i, cell in enumerate(self.cells):\n",
        "      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n",
        "      if i == 2*self._layers//3:\n",
        "        if self._auxiliary and self.training:\n",
        "          logits_aux = self.auxiliary_head(s1)\n",
        "    out = self.global_pooling(s1)\n",
        "    logits = self.classifier(out.view(out.size(0),-1))\n",
        "    return logits, logits_aux\n",
        "\n",
        "\n",
        "class NetworkImageNet(nn.Module):\n",
        "\n",
        "  def __init__(self, C, num_classes, layers, auxiliary, genotype):\n",
        "    super(NetworkImageNet, self).__init__()\n",
        "    self._layers = layers\n",
        "    self._auxiliary = auxiliary\n",
        "\n",
        "    self.stem0 = nn.Sequential(\n",
        "      nn.Conv2d(3, C // 2, kernel_size=3, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(C // 2),\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(C // 2, C, 3, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(C),\n",
        "    )\n",
        "\n",
        "    self.stem1 = nn.Sequential(\n",
        "      nn.ReLU(inplace=True),\n",
        "      nn.Conv2d(C, C, 3, stride=2, padding=1, bias=False),\n",
        "      nn.BatchNorm2d(C),\n",
        "    )\n",
        "\n",
        "    C_prev_prev, C_prev, C_curr = C, C, C\n",
        "\n",
        "    self.cells = nn.ModuleList()\n",
        "    reduction_prev = True\n",
        "    for i in range(layers):\n",
        "      if i in [layers // 3, 2 * layers // 3]:\n",
        "        C_curr *= 2\n",
        "        reduction = True\n",
        "      else:\n",
        "        reduction = False\n",
        "      cell = Cell(genotype, C_prev_prev, C_prev, C_curr, reduction, reduction_prev)\n",
        "      reduction_prev = reduction\n",
        "      self.cells += [cell]\n",
        "      C_prev_prev, C_prev = C_prev, cell.multiplier * C_curr\n",
        "      if i == 2 * layers // 3:\n",
        "        C_to_auxiliary = C_prev\n",
        "\n",
        "    if auxiliary:\n",
        "      self.auxiliary_head = AuxiliaryHeadImageNet(C_to_auxiliary, num_classes)\n",
        "    self.global_pooling = nn.AvgPool2d(7)\n",
        "    self.classifier = nn.Linear(C_prev, num_classes)\n",
        "\n",
        "  def forward(self, input):\n",
        "    logits_aux = None\n",
        "    s0 = self.stem0(input)\n",
        "    s1 = self.stem1(s0)\n",
        "    for i, cell in enumerate(self.cells):\n",
        "      s0, s1 = s1, cell(s0, s1, self.drop_path_prob)\n",
        "      if i == 2 * self._layers // 3:\n",
        "        if self._auxiliary and self.training:\n",
        "          logits_aux = self.auxiliary_head(s1)\n",
        "    out = self.global_pooling(s1)\n",
        "    logits = self.classifier(out.view(out.size(0), -1))\n",
        "    return logits, logits_aux\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qvQPw5gadNO1"
      },
      "source": [
        "# **train.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abIusLJsc-ga",
        "outputId": "0852d69a-8a35-4509-fe3f-c978bf8dea8c"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "# import utils\n",
        "import logging\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "# import genotypes\n",
        "import torch.utils\n",
        "import torchvision.datasets as dset\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from torch.autograd import Variable\n",
        "# from model import NetworkCIFAR as Network\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"cifar\")\n",
        "parser.add_argument('--data', type=str, default='../data', help='location of the data corpus')\n",
        "# parser.add_argument('--batch_size', type=int, default=96, help='batch size')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
        "parser.add_argument('--learning_rate', type=float, default=0.025, help='init learning rate')\n",
        "parser.add_argument('--momentum', type=float, default=0.9, help='momentum')\n",
        "parser.add_argument('--weight_decay', type=float, default=3e-4, help='weight decay')\n",
        "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
        "# parser.add_argument('--epochs', type=int, default=600, help='num of training epochs')\n",
        "# parser.add_argument('--epochs', type=int, default=30, help='num of training epochs')\n",
        "parser.add_argument('--epochs', type=int, default=1, help='num of training epochs')\n",
        "parser.add_argument('--init_channels', type=int, default=36, help='num of init channels')\n",
        "parser.add_argument('--layers', type=int, default=20, help='total number of layers')\n",
        "parser.add_argument('--model_path', type=str, default='saved_models', help='path to save the model')\n",
        "parser.add_argument('--auxiliary', action='store_true', default=False, help='use auxiliary tower')\n",
        "parser.add_argument('--auxiliary_weight', type=float, default=0.4, help='weight for auxiliary loss')\n",
        "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
        "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
        "parser.add_argument('--drop_path_prob', type=float, default=0.2, help='drop path probability')\n",
        "parser.add_argument('--save', type=str, default='EXP', help='experiment name')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
        "parser.add_argument('--arch', type=str, default='DARTS', help='which architecture to use')\n",
        "parser.add_argument('--grad_clip', type=float, default=5, help='gradient clipping')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "args.save = 'eval-{}-{}'.format(args.save, time.strftime(\"%Y%m%d-%H%M%S\"))\n",
        "create_exp_dir(args.save, scripts_to_save=glob.glob('*.py'))\n",
        "\n",
        "log_format = '%(asctime)s %(message)s'\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "fh = logging.FileHandler(os.path.join(args.save, 'log.txt'))\n",
        "fh.setFormatter(logging.Formatter(log_format))\n",
        "logging.getLogger().addHandler(fh)\n",
        "\n",
        "CIFAR_CLASSES = 10\n",
        "\n",
        "\n",
        "def main():\n",
        "  if not torch.cuda.is_available():\n",
        "    logging.info('no gpu device available')\n",
        "    sys.exit(1)\n",
        "\n",
        "  np.random.seed(args.seed)\n",
        "  torch.cuda.set_device(args.gpu)\n",
        "  cudnn.benchmark = True\n",
        "  torch.manual_seed(args.seed)\n",
        "  cudnn.enabled=True\n",
        "  torch.cuda.manual_seed(args.seed)\n",
        "  logging.info('gpu device = %d' % args.gpu)\n",
        "  logging.info(\"args = %s\", args)\n",
        "\n",
        "  # genotypesは、import genotypesによるもの。\n",
        "  # evalは(\"計算式\")を計算する関数。\n",
        "\n",
        "  # %sは %演算子であり、文字列(str()で変換)、 % 以下の文字列をsに対して代入している。\n",
        "  # 今回は args.arch = DARTSより　genotypes.DARTSという文字が入る。\n",
        "\n",
        "  #genotypesは、namedtupleとなっており、\n",
        "\n",
        "  #   >>> Car = namedtuple('Car', [\n",
        "  # ...     'color',\n",
        "  # ...     'mileage',\n",
        "  # ... ])\n",
        "  # >>> my_car = Car('red', 3812.4)\n",
        "  # >>> my_car.color\n",
        "  # 'red'\n",
        "  # >>> my_car.mileage\n",
        "  # 3812.4\n",
        "\n",
        "  # Genotype = namedtuple('Genotype', 'normal normal_concat reduce reduce_concat')が設定されており,(恐らく)初期設定値が入っている。\n",
        "  # genotypeに文字列が含まれないようにevalとして渡している。\n",
        "\n",
        "\n",
        "  # genotype = eval(\"genotypes.%s\" % args.arch)\n",
        "  model = NetworkCIFAR(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)\n",
        "  model = model.cuda()\n",
        "\n",
        "  logging.info(\"param size = %fMB\", count_parameters_in_MB(model))\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.cuda()\n",
        "  optimizer = torch.optim.SGD(\n",
        "      model.parameters(),\n",
        "      args.learning_rate,\n",
        "      momentum=args.momentum,\n",
        "      weight_decay=args.weight_decay\n",
        "      )\n",
        "\n",
        "  train_transform, valid_transform = _data_transforms_cifar10(args)\n",
        "  train_data = dset.CIFAR10(root=args.data, train=True, download=True, transform=train_transform)\n",
        "  valid_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=valid_transform)\n",
        "\n",
        "  train_queue = torch.utils.data.DataLoader(\n",
        "      train_data, batch_size=args.batch_size, shuffle=True, pin_memory=True, num_workers=2)\n",
        "\n",
        "  valid_queue = torch.utils.data.DataLoader(\n",
        "      valid_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, float(args.epochs))\n",
        "\n",
        "  for epoch in range(args.epochs):\n",
        "    scheduler.step()\n",
        "    logging.info('epoch %d lr %e', epoch, scheduler.get_lr()[0])\n",
        "    model.drop_path_prob = args.drop_path_prob * epoch / args.epochs\n",
        "\n",
        "    train_acc, train_obj = train(train_queue, model, criterion, optimizer)\n",
        "    logging.info('train_acc %f', train_acc)\n",
        "\n",
        "    valid_acc, valid_obj = infer(valid_queue, model, criterion)\n",
        "    logging.info('valid_acc %f', valid_acc)\n",
        "\n",
        "    save(model, os.path.join(args.save, 'weights.pt'))\n",
        "\n",
        "\n",
        "def train(train_queue, model, criterion, optimizer):\n",
        "  objs = AvgrageMeter()\n",
        "  top1 = AvgrageMeter()\n",
        "  top5 = AvgrageMeter()\n",
        "  model.train()\n",
        "\n",
        "  for step, (input, target) in enumerate(train_queue):\n",
        "    input = Variable(input).cuda()\n",
        "    target = Variable(target).cuda(async=True)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    logits, logits_aux = model(input)\n",
        "    loss = criterion(logits, target)\n",
        "    if args.auxiliary:\n",
        "      loss_aux = criterion(logits_aux, target)\n",
        "      loss += args.auxiliary_weight*loss_aux\n",
        "    loss.backward()\n",
        "    nn.utils.clip_grad_norm(model.parameters(), args.grad_clip)\n",
        "    optimizer.step()\n",
        "\n",
        "    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "    n = input.size(0)\n",
        "    objs.update(loss.data.item(), n)\n",
        "    top1.update(prec1.data.item(), n)\n",
        "    top5.update(prec5.data.item(), n)\n",
        "\n",
        "    if step % args.report_freq == 0:\n",
        "      logging.info('train %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
        "\n",
        "  return top1.avg, objs.avg\n",
        "\n",
        "\n",
        "def infer(valid_queue, model, criterion):\n",
        "  objs = AvgrageMeter()\n",
        "  top1 = AvgrageMeter()\n",
        "  top5 = AvgrageMeter()\n",
        "  model.eval()\n",
        "\n",
        "  for step, (input, target) in enumerate(valid_queue):\n",
        "    input = Variable(input, volatile=True).cuda()\n",
        "    target = Variable(target, volatile=True).cuda(async=True)\n",
        "\n",
        "    logits, _ = model(input)\n",
        "    loss = criterion(logits, target)\n",
        "\n",
        "    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "    n = input.size(0)\n",
        "    objs.update(loss.data.item(), n)\n",
        "    top1.update(prec1.data.item(), n)\n",
        "    top5.update(prec5.data.item(), n)\n",
        "\n",
        "    if step % args.report_freq == 0:\n",
        "      logging.info('valid %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
        "\n",
        "  return top1.avg, objs.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Experiment dir : eval-EXP-20210108-124613\n",
            "01/08 12:46:13 PM gpu device = 0\n",
            "01/08 12:46:13 PM args = Namespace(arch='DARTS', auxiliary=False, auxiliary_weight=0.4, batch_size=32, cutout=False, cutout_length=16, data='../data', drop_path_prob=0.2, epochs=1, gpu=0, grad_clip=5, init_channels=36, layers=20, learning_rate=0.025, model_path='saved_models', momentum=0.9, report_freq=50, save='eval-EXP-20210108-124613', seed=0, weight_decay=0.0003)\n",
            "108 108 36\n",
            "108 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 72\n",
            "144 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 144\n",
            "288 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "01/08 12:46:13 PM param size = 2.967238MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "01/08 12:46:15 PM epoch 0 lr 0.000000e+00\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:136: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:509: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  \"please use `get_last_lr()`.\", UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:46:15 PM train 000 2.199876e+00 3.125000 68.750000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:153: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:46:26 PM train 050 2.370465e+00 11.458333 51.041667\n",
            "01/08 12:46:36 PM train 100 2.378709e+00 10.550743 50.247525\n",
            "01/08 12:46:47 PM train 150 2.375889e+00 11.175497 50.186258\n",
            "01/08 12:46:57 PM train 200 2.371212e+00 11.069652 50.699627\n",
            "01/08 12:47:08 PM train 250 2.374375e+00 10.943725 50.647410\n",
            "01/08 12:47:18 PM train 300 2.373662e+00 10.714286 50.685216\n",
            "01/08 12:47:28 PM train 350 2.375482e+00 10.630342 50.543091\n",
            "01/08 12:47:39 PM train 400 2.375891e+00 10.629676 50.561097\n",
            "01/08 12:47:49 PM train 450 2.376600e+00 10.636086 50.665188\n",
            "01/08 12:47:59 PM train 500 2.376043e+00 10.610030 50.711078\n",
            "01/08 12:48:10 PM train 550 2.376124e+00 10.690789 50.776996\n",
            "01/08 12:48:20 PM train 600 2.377257e+00 10.570923 50.774750\n",
            "01/08 12:48:31 PM train 650 2.377261e+00 10.613479 50.753648\n",
            "01/08 12:48:42 PM train 700 2.376719e+00 10.614301 50.744472\n",
            "01/08 12:48:52 PM train 750 2.376938e+00 10.602530 50.790613\n",
            "01/08 12:49:03 PM train 800 2.376276e+00 10.537609 50.788077\n",
            "01/08 12:49:14 PM train 850 2.376683e+00 10.528055 50.741774\n",
            "01/08 12:49:24 PM train 900 2.377199e+00 10.502220 50.728357\n",
            "01/08 12:49:35 PM train 950 2.376669e+00 10.561251 50.768927\n",
            "01/08 12:49:45 PM train 1000 2.375518e+00 10.639361 50.871004\n",
            "01/08 12:49:56 PM train 1050 2.374193e+00 10.724905 50.993102\n",
            "01/08 12:50:06 PM train 1100 2.373286e+00 10.743074 51.078565\n",
            "01/08 12:50:18 PM train 1150 2.374031e+00 10.762381 51.034427\n",
            "01/08 12:50:28 PM train 1200 2.374317e+00 10.733243 51.061615\n",
            "01/08 12:50:38 PM train 1250 2.373717e+00 10.768885 51.156575\n",
            "01/08 12:50:49 PM train 1300 2.374907e+00 10.753747 51.109723\n",
            "01/08 12:50:59 PM train 1350 2.375263e+00 10.760548 51.054774\n",
            "01/08 12:51:10 PM train 1400 2.375603e+00 10.753480 51.017131\n",
            "01/08 12:51:21 PM train 1450 2.375596e+00 10.792126 51.070382\n",
            "01/08 12:51:32 PM train 1500 2.375452e+00 10.778231 51.084694\n",
            "01/08 12:51:42 PM train 1550 2.374762e+00 10.785380 51.092037\n",
            "01/08 12:51:45 PM train_acc 10.788000\n",
            "01/08 12:51:45 PM valid 000 2.456669e+00 6.250000 37.500000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:175: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:176: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:51:47 PM valid 050 2.443318e+00 10.110294 49.877451\n",
            "01/08 12:51:50 PM valid 100 2.424396e+00 10.519802 50.185644\n",
            "01/08 12:51:52 PM valid 150 2.415442e+00 9.850993 50.807119\n",
            "01/08 12:51:55 PM valid 200 2.406560e+00 9.794776 50.824005\n",
            "01/08 12:51:57 PM valid 250 2.396475e+00 10.209163 50.996016\n",
            "01/08 12:51:59 PM valid 300 2.398997e+00 10.184801 50.384136\n",
            "01/08 12:52:00 PM valid_acc 10.220000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ZCAjGoJlbxF",
        "outputId": "349e53a7-c572-4c2e-db00-6d3db89f7d50"
      },
      "source": [
        "genotype"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Genotype(normal=[('dil_conv_5x5', 1), ('sep_conv_3x3', 0), ('dil_conv_3x3', 2), ('dil_conv_5x5', 1), ('avg_pool_3x3', 1), ('dil_conv_5x5', 2), ('dil_conv_3x3', 0), ('max_pool_3x3', 3)], normal_concat=range(2, 6), reduce=[('avg_pool_3x3', 1), ('sep_conv_5x5', 0), ('dil_conv_5x5', 0), ('sep_conv_3x3', 1), ('max_pool_3x3', 0), ('avg_pool_3x3', 2), ('sep_conv_3x3', 4), ('sep_conv_3x3', 0)], reduce_concat=range(2, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Bu-fiZgWfgs"
      },
      "source": [
        "# **test.py**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2LAYmYmPsWW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d09216e1-0c02-4589-bada-b2ca9fdf5e1f"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.utils\n",
        "import torchvision.datasets as dset\n",
        "import torch.backends.cudnn as cudnn\n",
        "\n",
        "from torch.autograd import Variable\n",
        "# from model import NetworkCIFAR\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser(\"cifar\")\n",
        "parser.add_argument('--data', type=str, default='./data', help='location of the data corpus')\n",
        "# parser.add_argument('--batch_size', type=int, default=96, help='batch size')\n",
        "parser.add_argument('--batch_size', type=int, default=32, help='batch size')\n",
        "parser.add_argument('--report_freq', type=float, default=50, help='report frequency')\n",
        "parser.add_argument('--gpu', type=int, default=0, help='gpu device id')\n",
        "parser.add_argument('--init_channels', type=int, default=36, help='num of init channels')\n",
        "parser.add_argument('--layers', type=int, default=20, help='total number of layers')\n",
        "parser.add_argument('--model_path', type=str, default='eval-EXP-20210108-124613/weights.pt', help='path of pretrained model')\n",
        "parser.add_argument('--auxiliary', action='store_true', default=False, help='use auxiliary tower')\n",
        "parser.add_argument('--cutout', action='store_true', default=False, help='use cutout')\n",
        "parser.add_argument('--cutout_length', type=int, default=16, help='cutout length')\n",
        "parser.add_argument('--drop_path_prob', type=float, default=0.2, help='drop path probability')\n",
        "parser.add_argument('--seed', type=int, default=0, help='random seed')\n",
        "parser.add_argument('--arch', type=str, default='DARTS', help='which architecture to use')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "log_format = '%(asctime)s %(message)s'\n",
        "logging.basicConfig(stream=sys.stdout, level=logging.INFO,\n",
        "    format=log_format, datefmt='%m/%d %I:%M:%S %p')\n",
        "\n",
        "CIFAR_CLASSES = 10\n",
        "\n",
        "def main():\n",
        "  if not torch.cuda.is_available():\n",
        "    logging.info('no gpu device available')\n",
        "    sys.exit(1)\n",
        "  np.random.seed(args.seed)\n",
        "  torch.cuda.set_device(args.gpu)\n",
        "  cudnn.benchmark = True\n",
        "  torch.manual_seed(args.seed)\n",
        "  cudnn.enabled=True\n",
        "  torch.cuda.manual_seed(args.seed)\n",
        "  logging.info('gpu device = %d' % args.gpu)\n",
        "  logging.info(\"args = %s\", args)\n",
        "\n",
        "  # genotype = genotype\n",
        "  model = NetworkCIFAR(args.init_channels, CIFAR_CLASSES, args.layers, args.auxiliary, genotype)\n",
        "  model = model.cuda()\n",
        "  load(model, args.model_path)\n",
        "\n",
        "  logging.info(\"param size = %fMB\", count_parameters_in_MB(model))\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.cuda()\n",
        "\n",
        "  _, test_transform = _data_transforms_cifar10(args)\n",
        "  test_data = dset.CIFAR10(root=args.data, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  test_queue = torch.utils.data.DataLoader(\n",
        "      test_data, batch_size=args.batch_size, shuffle=False, pin_memory=True, num_workers=2)\n",
        "\n",
        "  model.drop_path_prob = args.drop_path_prob\n",
        "  test_acc, test_obj = infer(test_queue, model, criterion)\n",
        "  logging.info('test_acc %f', test_acc)\n",
        "\n",
        "\n",
        "def infer(test_queue, model, criterion):\n",
        "  objs = AvgrageMeter()\n",
        "  top1 = AvgrageMeter()\n",
        "  top5 = AvgrageMeter()\n",
        "  model.eval()\n",
        "\n",
        "  for step, (input, target) in enumerate(test_queue):\n",
        "    input = Variable(input, volatile=True).cuda()\n",
        "    target = Variable(target, volatile=True).cuda(async=True)\n",
        "\n",
        "    logits, _ = model(input)\n",
        "    loss = criterion(logits, target)\n",
        "\n",
        "    prec1, prec5 = accuracy(logits, target, topk=(1, 5))\n",
        "    n = input.size(0)\n",
        "    objs.update(loss.data.item(), n)\n",
        "    top1.update(prec1.data.item(), n)\n",
        "    top5.update(prec5.data.item(), n)\n",
        "\n",
        "    if step % args.report_freq == 0:\n",
        "      logging.info('test %03d %e %f %f', step, objs.avg, top1.avg, top5.avg)\n",
        "\n",
        "  return top1.avg, objs.avg\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main() "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "01/08 12:53:06 PM gpu device = 0\n",
            "01/08 12:53:06 PM args = Namespace(arch='DARTS', auxiliary=False, batch_size=32, cutout=False, cutout_length=16, data='./data', drop_path_prob=0.2, gpu=0, init_channels=36, layers=20, model_path='eval-EXP-20210108-124613/weights.pt', report_freq=50, seed=0)\n",
            "108 108 36\n",
            "108 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 36\n",
            "144 144 72\n",
            "144 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 72\n",
            "288 288 144\n",
            "288 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "576 576 144\n",
            "01/08 12:53:06 PM param size = 2.967238MB\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "01/08 12:53:07 PM test 000 2.456669e+00 6.250000 37.500000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:83: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "01/08 12:53:09 PM test 050 2.443318e+00 10.110294 49.877451\n",
            "01/08 12:53:12 PM test 100 2.424396e+00 10.519802 50.185644\n",
            "01/08 12:53:14 PM test 150 2.415442e+00 9.850993 50.807119\n",
            "01/08 12:53:16 PM test 200 2.406560e+00 9.794776 50.824005\n",
            "01/08 12:53:18 PM test 250 2.396475e+00 10.209163 50.996016\n",
            "01/08 12:53:21 PM test 300 2.398997e+00 10.184801 50.384136\n",
            "01/08 12:53:21 PM test_acc 10.220000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpaH10d33FlS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}